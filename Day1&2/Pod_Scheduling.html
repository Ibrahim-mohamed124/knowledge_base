<!--
title: Pod Scheduling
description: 
published: true
date: 2025-11-27T17:08:56.835Z
tags: 
editor: ckeditor
dateCreated: 2025-11-17T09:35:08.089Z
-->

<h3>OpenShift includes the following advanced scheduling features:</h3>
<ul>
  <li><mark class="marker-yellow">Scheduler profiles</mark>, to control how OpenShift schedules pods on nodes.</li>
  <li><mark class="marker-yellow">Pod affinity rules</mark>, to keep sets of pods close to each other, on the same nodes. For example, you can run a REST service and its database on the same node to minimize network latency.</li>
  <li><mark class="marker-yellow">Pod anti-affinity rules</mark>, to keep sets of pods far away from each other, on different nodes. For example, you can run replica pods of the same deployment in different nodes, so that if a node fails, then you do not lose all the pods for the workload.</li>
  <li><mark class="marker-yellow">Node affinity,</mark> to keep sets of pods running on the same group of nodes. For example, you can configure a pod to run on nodes with a specific CPU.</li>
  <li><mark class="marker-yellow">Node selectors</mark>, to schedule pods to a specific set of nodes. For example, you can schedule a pod to a node that provides special hardware that the pod needs.<ul>
      <li>node selectors has the following levels: pod, project, and cluster-wide.</li>
      <li><mark class="marker-yellow">Pod node selectors are additive to cluster-wide and project node selectors.&nbsp;</mark></li>
    </ul>
  </li>
  <li><mark class="marker-yellow">Taints and tolerations</mark>, to avoid scheduling pods to a specific set of nodes. For example, you can block a pod to run on a node that is reserved for OpenShift cluster services or control plane services.</li>
</ul>
<pre><code class="language-plaintext"> oc adm taint nodes node1 key1=value1:NoSchedule</code></pre>
<ul>
  <li><mark class="marker-yellow">Pod disruption budget resource</mark>, which controls how many instances can be down at the same time during voluntary disruptions, such as when scaling down, updating applications, or draining a node for maintenance.</li>
</ul>
<figure class="image image_resized" style="width:100%;"><img src="/advanced-scheduler.svg"></figure>
<blockquote>
  <p>The default OpenShift pod scheduler determines the placement of pods onto nodes within the cluster. The scheduler reads data from the pod and identifies a suitable node based on configured profiles. After identifying the most suitable node, the scheduler creates a binding that associates the pod with a specific node, without modifying the pod.</p>
</blockquote>
<ul>
  <li>The OpenShift default pod scheduler determines the placement of a pod onto a particular node according to the following procedure:<ol>
      <li>Node filtering: The OpenShift scheduler<mark class="marker-yellow"> filters the nodes based on the configured constraints or requirements by means of functions called </mark><i><mark class="marker-yellow">predicates</mark></i><mark class="marker-yellow">.</mark> These predicates include available CPU capacity on the node to satisfy a pod's CPU resource request, free ports, or volume availability, among others.</li>
      <li>Prioritize the filtered list of nodes: In this step, the<mark class="marker-yellow"> scheduler assesses each node by using a set of priority or scoring functions, and assigns each node a score from 0 to 10</mark>. A score of 0 indicates a poor fit, and a score of 10 indicates an excellent fit for hosting the pod. Additionally, OpenShift administrators can assign a numeric weight to each scoring function in the scheduler's configuration. With this weight attribute, administrators can prioritize certain scoring functions.</li>
      <li>Select the best node: <mark class="marker-yellow">OpenShift sorts the nodes based on their scores and selects the node with the highest score.</mark> If multiple nodes receive the same score, then OpenShift selects one node randomly.</li>
      <li>If a pod does not specify its resource requests, then the scheduler could place it on a node that is already full, which could lead to poor performance or even killing the pod if the node is out of memory.</li>
    </ol>
  </li>
</ul>
<p>&nbsp;</p>
<h4>Scheduler Profiles</h4>
<pre><code class="language-plaintext">oc edit scheduler cluster &gt;&gt;&gt;&gt; spec.profile parameter.</code></pre>
<p>&nbsp;</p>
<ul>
  <li><code><strong>LowNodeUtilization</strong></code>
    <ul>
      <li>By using this profile<mark class="marker-yellow">, OpenShift distributes pods evenly across nodes to ensure a low resource usage per node.</mark> This profile provides the default scheduler behavior.</li>
    </ul>
  </li>
  <li><code><strong>HighNodeUtilization</strong></code>
    <ul>
      <li>With this profile,<mark class="marker-yellow"> OpenShift places as many pods as possible onto as few nodes as possible. </mark>This profile minimizes the node count but increases the resource usage per node.</li>
    </ul>
  </li>
  <li><code><strong>NoScoring</strong></code>
    <ul>
      <li>This profile aims for quick scheduling cycles by<mark class="marker-yellow"> disabling all the score plug-ins</mark>. Thus, by using this profile you sacrifice better scheduling decisions for faster ones.</li>
    </ul>
  </li>
</ul>
<p>&nbsp;</p>
<h4>Pod Disruption Budget</h4>
<ul>
  <li>policy resource to control the disruption of pods during voluntary disruptions, such as scaling down, updating applications, or draining a node for maintenance. Pod disruption budgets do not apply on node failures.</li>
  <li>manage the availability and stability of your applications during disruptions, to reduce the risk of service degradation or downtime when maintaining or updating your cluster.</li>
  <li>The configuration for a pod disruption budget consists of the following parts:<ul>
      <li>A label selector to target a specific set of pods. By using the label selector, you define the pods that the pod disruption budget protects. Only pods that match the label selector are subject to the budget constraints. The pod selector in a pod disruption budget typically selects only pods from the same deployment.</li>
      <li>The availability level, which specifies the minimum number of pods that must be available simultaneously. You can define the following availability levels:<ul>
          <li><code><strong>minAvailable</strong></code>
            <ul>
              <li>Defines the minimum number of pods that must always be available, even during a disruption. A pod is available when it has the <code>Ready</code> condition with the <code>True</code> value.</li>
            </ul>
          </li>
          <li><code><strong>maxUnavailable</strong></code>
            <ul>
              <li>Defines the maximum number of pods that can be unavailable during a disruption.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<h4>Cluster-wide Node Selectors</h4>
<ul>
  <li>specify the default node selectors that OpenShift adds to the pods.&nbsp;</li>
</ul>
<pre><code class="language-plaintext">oc edit scheduler cluster

apiVersion: config.openshift.io/v1
kind: Scheduler
metadata:
  name: cluster
...output omitted...
spec:
  defaultNodeSelector: key1=value1,key2=value2
...output omitted..</code></pre>
<blockquote>
  <p><code>openshift-kube-apiserver</code> project will redeploy.</p>
</blockquote>
<h4>Project Node Selectors</h4>
<ul>
  <li>OpenShift includes those node selectors to any new pods that are created in the project.&nbsp;</li>
</ul>
<pre><code class="language-plaintext">apiVersion: v1
kind: Namespace
metadata:
  name: myproject
  annotations:
    openshift.io/node-selector: key3=value3,key4=value4
...output omitted...</code></pre>
<blockquote>
  <p>Project node selectors take precedence over cluster-wide node selectors. Thus, if you define both project and cluster-wide node selectors, then OpenShift applies only the project node selectors to the new pods but not the cluster-wide node selectors.</p>
</blockquote>
<h4>Pod Scheduling and Node Conditions</h4>
<ul>
  <li>OpenShift automatically taints nodes under some conditions, such as memory or disk pressure.<ul>
      <li><code><strong>node.kubernetes.io/not-ready: </strong></code>The node is <mark class="marker-yellow">not ready.</mark> This taint corresponds to the <code>Ready=False</code> node condition.</li>
      <li><code><strong>node.kubernetes.io/unreachable: </strong></code>The node is<mark class="marker-yellow"> unreachable from the node controller.</mark> This taint corresponds to the <code>Ready=Unknown</code> node condition.</li>
      <li><code><strong>node.kubernetes.io/memory-pressure: </strong></code>The node has<mark class="marker-yellow"> memory pressure issues</mark>. This taint corresponds to the <code>MemoryPressure=True</code> node condition.</li>
      <li><code><strong>node.kubernetes.io/disk-pressure: </strong></code>The node has<mark class="marker-yellow"> disk pressure issues</mark>. This taint corresponds to the <code>DiskPressure=True</code> node condition.</li>
      <li><code><strong>node.kubernetes.io/network-unavailable: </strong></code>The<mark class="marker-yellow"> node network is unavailable.</mark></li>
      <li><code><strong>node.kubernetes.io/unschedulable: </strong></code>The node is unschedulable.</li>
      <li><code><strong>node.cloudprovider.kubernetes.io/uninitialized: </strong></code>This taint sets the node as unusable when you <mark class="marker-yellow">start the node controller in an external cloud provider</mark>. After the cloud controller manager initializes the node, the kubelet removes the taint.</li>
      <li><code><strong>node.kubernetes.io/pid-pressure: </strong></code>The node has <mark class="marker-yellow">process identifier (PID) pressure</mark>. This taint corresponds to the <code>PIDPressure=True</code> node condition.</li>
    </ul>
  </li>
</ul>
<h4>Project Tolerations</h4>
<ul>
  <li>OpenShift includes those tolerations to any new pods that are created in the project.&nbsp;</li>
</ul>
<pre><code class="language-plaintext">kind: Project
apiVersion: project.openshift.io/v1
metadata:
  name: myproject
  annotations:
    scheduler.alpha.kubernetes.io/defaultTolerations: &gt;-
      [{"operator":"Equal","effect":"NoSchedule","key":"key1","value":"value1"}]</code></pre>
<h4>Tolerating All Taints</h4>
<ul>
  <li>You can configure a pod to tolerate all the node taints by using the <code>operator: "Exists"</code> toleration with no key or value parameters.</li>
</ul>
<pre><code class="language-plaintext">apiVersion: v1
kind: Pod
metadata:
  name: my-pod

...output omitted...

spec:
	tolerations:
	- operator: "Exists"</code></pre>
<h4>Topology Keys</h4>
<ul>
  <li>The Kubernetes scheduler can use node labels to indicate the failure domain that each node belongs to.<ul>
      <li><code><strong>kubernetes.io/hostname: </strong></code>This label is set to match the hostname value of each node in the cluster.</li>
      <li><code><strong>topology.kubernetes.io/zone: </strong></code>Represents the availability zone of the node. This label is set by the cloud provider and might not be present in on-premise deployments.</li>
      <li><code><strong>topology.kubernetes.io/region: </strong></code>Represents the region of the node. This label is set by the cloud provider and might not be present in on-premise deployments.</li>
    </ul>
  </li>
</ul>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
