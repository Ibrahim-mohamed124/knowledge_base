<!--
title: Logging
description: 
published: true
date: 2025-11-30T09:12:32.360Z
tags: 
editor: ckeditor
dateCreated: 2025-11-28T11:57:32.900Z
-->

<h1>OpenShift Logging</h1>
<ul>
  <li>OpenShift Logging collects and aggregates the log messages from all the pods and nodes in your cluster.</li>
  <li>Configure OpenShift Logging to forward the logs to a third-party log aggregator for long-term storage, or to an observability platform for further analysis, and minimize the resource requirement on your cluster.<ul>
      <li>Elasticsearch</li>
      <li>Grafana Loki</li>
      <li>Splunk</li>
      <li>Amazon CloudWatch</li>
      <li>Google Cloud Logging</li>
    </ul>
  </li>
</ul>
<h3>OpenShift Logging Components</h3>
<ul>
  <li>A collector, a log store, and a visualization console.</li>
</ul>
<figure class="image image_resized" style="width:99.05%;"><img src="/openshift_logging_arch.svg"></figure>
<p>&nbsp;</p>
<h4>Log Collector</h4>
<ul>
  <li>OpenShift Logging uses <mark class="marker-yellow">Vector </mark>to collect logs from all running containers and cluster nodes, and<mark class="marker-yellow"> replaces Fluentd</mark>, which was the collector in earlier versions of OpenShift Logging.</li>
  <li>Vector<mark class="marker-yellow"> runs as a daemon set</mark> in the cluster and therefore runs on all nodes.</li>
  <li>Vector groups logs into:<ul>
      <li><strong>Infrastructure: </strong>Infrastructure logs include <mark class="marker-yellow">container logs in</mark> the <code>openshift-*</code>, <code>kube*</code>, and <code>default</code> namespaces, and <mark class="marker-yellow">system logs from the cluster nodes.</mark></li>
      <li><strong>Audit: </strong>Audit logs include both <mark class="marker-yellow">Kubernetes API and OpenShift API audit logs</mark>, as well as the<mark class="marker-yellow"> Linux audit logs from the cluster nodes.</mark> These logs might contain sensitive security details, and OpenShift Logging does not store them by default.</li>
      <li><strong>Application: </strong><mark class="marker-yellow">Application logs are all container logs from user projects.</mark></li>
    </ul>
  </li>
</ul>
<h4>Log Store</h4>
<ul>
  <li>The log store uses Grafana Loki to aggregate logs from the entire cluster into a central place and provides access control to logs.<mark class="marker-yellow"> Loki replaces Elasticsearch</mark>, which was the log store in earlier versions of the logging subsystem.</li>
</ul>
<h4>Visualization</h4>
<ul>
  <li>OpenShift Logging provides a native OpenShift Console plug-in to view and query logs in the internal log store. The OpenShift Logging UI component replaces <mark class="marker-yellow">Kibana</mark>, which was the web interface in earlier versions of OpenShift Logging.</li>
</ul>
<h3>Configure OpenShift Logging Components</h3>
<ul>
  <li>The <code>ClusterLogging</code> CR in <mark class="marker-yellow">openshift-logging namespace</mark> configures and manages deploying the OpenShift Logging components.</li>
</ul>
<pre><code class="language-plaintext">apiVersion: logging.openshift.io/v1
kind: ClusterLogging
metadata:
  name: instance 1
  namespace: openshift-logging
spec:
  managementState: Managed 2
  collection:
    type: vector 3
    tolerations: 
    - effect: NoSchedule
      key: node-role.kubernetes.io/infra
      value: reserved
    - effect: NoExecute
      key: node-role.kubernetes.io/infra
      value: reserved</code></pre>
<p>&nbsp;</p>
<figure class="table">
  <table style="background-color:rgb(255, 255, 255);">
    <tbody>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg" alt="1"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">The resource name must be <code>instance</code>.</td>
      </tr>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg" alt="2"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">The management state must be <code>Managed</code> for the components to receive updates from the OpenShift Logging operator.</td>
      </tr>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg" alt="3"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">The collector type to deploy. It can be either <code>vector</code> or the deprecated <code>fluentd</code> collector.</td>
      </tr>
    </tbody>
  </table>
</figure>
<blockquote>
  <p>OpenShift Logging automatically adds the <code>node-role.kubernetes.io/master:NoSchedule</code> toleration to the collector daemon set so the collector can run on both control planes and compute nodes.</p>
</blockquote>
<ul>
  <li><mark class="marker-pink">The log collector pods are deployed only if the internal log store is enabled, or if log forwarding to an external log aggregator is configured.</mark></li>
</ul>
<h3>Configure Log Collection and Forwarding</h3>
<ul>
  <li>The log collector is configured by default to forward infrastructure and application logs to the internal log store that you define in the cluster logging resource.</li>
  <li>Vector can forward logs to various log stores, in addition to the internal Loki deployment, such as Splunk, Amazon CloudWatch, or <mark class="marker-yellow">any logging solution that uses the syslog protocol.</mark></li>
  <li>The ClusterLogForwarder custom resource configures the log collector, and defines which logs to collect and where to send them.</li>
</ul>
<pre><code class="language-plaintext">apiVersion: logging.openshift.io/v1
kind: ClusterLogForwarder
metadata:
  name: instance 1
  namespace: openshift-logging
spec:
  outputs:
    - name: splunk-receiver 2
      secret: 3
        name: splunk-auth-token
      type: splunk 4
      url: https://mysplunkserver.example.com:8088/services/collector 5
  pipelines:
    - name: to-splunk 6
      inputRefs:
        - audit
        - infrastructure
      outputRefs:
        - splunk-receiver</code></pre>
<p>&nbsp;</p>
<figure class="table">
  <table style="background-color:rgb(255, 255, 255);">
    <tbody>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg" alt="1"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">The resource name must be <code>instance</code> and in the <code>openshift-logging</code> namespace.</td>
      </tr>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg" alt="2"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">Name of the log output. You use that name in the log pipeline to refer to that log destination.</td>
      </tr>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg" alt="3"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">Secret to use for connecting to the log store, if required. In this example, the secret contains the authentication token for the Splunk instance.</td>
      </tr>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/4.svg" alt="4"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">Type of the external log store.</td>
      </tr>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/5.svg" alt="5"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">URL of the external log store.</td>
      </tr>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/6.svg" alt="6"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">Log pipeline configuration that forwards audit and infrastructure logs to the Splunk instance.</td>
      </tr>
    </tbody>
  </table>
</figure>
<ul>
  <li>The cluster log forwarder resource is composed of inputs, outputs, and pipelines.<ol>
      <li><code><strong>inputs:</strong></code>An input defines the log type to collect. OpenShift Logging provides a predefined input for each log category: <code>infrastructure</code>, <code>audit</code>, and <code>application</code>.</li>
      <li><code><strong>outputs: </strong></code>An output defines a destination for the logs. You can configure multiple log outputs to one or more external log stores.<ul>
          <li>If the internal log store is configured, then the <code>default</code> output becomes available to target the internal Loki instance.</li>
        </ul>
      </li>
      <li><code><strong>pipelines: </strong></code>A pipeline<mark class="marker-yellow"> defines a routing from one or more log inputs to one or more log outputs</mark>. You can create several log pipelines and use any combination of log inputs and outputs to forward logs according to your needs.</li>
    </ol>
  </li>
</ul>
<h4>Collect Kubernetes Events</h4>
<ul>
  <li>The Event Router is an optional OpenShift Logging component that you can deploy to log Kubernetes events.</li>
  <li>The Event Router monitors the OpenShift event API and sends the events to the container stdout.</li>
  <li>The log collector captures the Event Router container logs and forwards them to the log store through the <code>infrastructure</code> category.</li>
</ul>
<h4>Filtering Log</h4>
<ul>
  <li>OpenShift Logging provides filtering capabilities to limit the number of logs to forward to the log store.</li>
</ul>
<pre><code class="language-plaintext">apiVersion: logging.openshift.io/v1
kind: ClusterLogForwarder
metadata:
  name: instance
  namespace: openshift-logging
spec:
  inputs:
  - name: production-apps 1
    application:
      selector:
        matchLabels:
          environment: production
  - name: qa-chain 2
    application:
      selector:
        matchLabels:
          environment: development
      namespaces:
      - qa-testing
      - builders
  ...output omitted...
  pipelines:
  - name: to-splunk 3
    inputRefs:
      - qa-chain
      - production-apps
    outputRefs:
      - splunk-receiver</code></pre>
<p>&nbsp;</p>
<figure class="table">
  <table style="background-color:rgb(255, 255, 255);">
    <tbody>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg" alt="1"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">The <code>production-apps</code> log input collects application logs from pods with the <code>environment: production</code> label.</td>
      </tr>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg" alt="2"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">The <code>qa-chain</code> log input collects application logs from pods with the <code>environment: development</code> label in the <code>qa-testing</code> and <code>builders</code> projects.</td>
      </tr>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg" alt="3"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">The log pipeline forwards both <code>qa-chain</code> and <code>production-apps</code> log inputs to the Splunk instance.</td>
      </tr>
    </tbody>
  </table>
</figure>
<ul>
  <li>Each call to the Kubernetes and OpenShift APIs generates an audit event that is forwarded to the log store.</li>
  <li>Define audit filters that remove unwanted or low-value events from the audit logs, and reduce the data that is sent to the log store.</li>
</ul>
<pre><code class="language-plaintext">spec:
  filters:
  - name: unwanted-events 1
    type: kubeAPIAudit
    kubeAPIAudit:
      rules:
      - level: None 2
        namespaces:
        - openshift-*
        - kube*
        userGroups:
        - system:serviceaccounts:openshift-*
        - system:nodes
      - level: None 3
        resources:
        - group: coordination.k8s.io
          resources:
          - leases
        users:
        - system:kube*
        - system:apiserver
        verbs:
        - update
  ...output omitted...
  pipelines:
  - name: audit-to-syslog
    inputRefs:
    - audit
    filterRefs: 4
    - unwanted-events
    outputRefs:
    - audit-rsyslog</code></pre>
<p>&nbsp;</p>
<figure class="table">
  <table style="background-color:rgb(255, 255, 255);">
    <tbody>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg" alt="1"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">Name of the filter. You use that name in the log pipeline to refer to that filter configuration.</td>
      </tr>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg" alt="2"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">The first rule removes all audit events that were created from users in the <code>system:serviceaccounts:openshift-*</code> and <code>system:nodes</code> groups, for resources in the <code>openshift-*</code> and <code>kube*</code> namespaces.</td>
      </tr>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg" alt="3"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">The second rule removes audit events that were generated from the <code>system:kube*</code> and <code>system:apiserver</code> users, who update <code>leases</code> resources in the <code>coordination.k8s.io</code> API group.</td>
      </tr>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/4.svg" alt="4"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">The <code>filterRefs</code> field lists all filters to apply to the log pipeline.</td>
      </tr>
    </tbody>
  </table>
</figure>
<h2>Centralized Logging</h2>
<ul>
  <li>By default, Kubernetes stores logs from pods in the local disk on the nodes, and the information is lost when the pod is deleted.</li>
  <li>Kubernetes also removes the logs when a node is restarted, for example after applying an update.</li>
</ul>
<h3>Loki Log Store</h3>
<ul>
  <li>Loki indexes <mark class="marker-yellow">only a few fixed labels during ingestion</mark>, and then<mark class="marker-yellow"> compresses and stores the log data in chunks in object stores.&nbsp;</mark></li>
  <li>The Loki operator includes the <code>LokiStack</code> CR to manage the Loki deployment.</li>
  <li>The steps for using Loki as the log store for OpenShift Logging are as follows:<ol>
      <li>Install the Loki operator.</li>
      <li>Configure the Loki object storage.</li>
      <li>Create a <code>LokiStack</code> CR with the object storage credentials.</li>
      <li>Configure the <code>ClusterLogging</code> CR from OpenShift Logging to use Loki as the log store.</li>
    </ol>
  </li>
  <li>Creating Object Storage from ODF:</li>
</ul>
<pre><code class="language-plaintext">apiVersion: objectbucket.io/v1alpha1
kind: ObjectBucketClaim
metadata:
  name: loki-bucket-odf
  namespace: openshift-logging
spec:
  generateBucketName: loki-bucket-odf
  storageClassName: openshift-storage.noobaa.io</code></pre>
<pre><code class="language-plaintext">oc get configmap/loki-bucket-odf -o yaml
apiVersion: v1
data:
  BUCKET_HOST: s3.openshift-storage.svc
  BUCKET_NAME: loki-bucket-odf-69d9...ab91
  BUCKET_PORT: "443"
  BUCKET_REGION: ""
  BUCKET_SUBREGION: ""
kind: ConfigMap
...output omitted...</code></pre>
<pre><code class="language-plaintext"> oc extract --to=- secret/loki-bucket-odf -n openshift-logging
# AWS_ACCESS_KEY_ID
o0sOY8oaZtPfP18DRjSa
# AWS_SECRET_ACCESS_KEY
z1QtDH37lUvLnkjpE3E4aS8yQI56CLPozGJOt31e</code></pre>
<pre><code class="language-plaintext"> oc create secret generic logging-loki-odf \
  -n openshift-logging \
  --from-literal=access_key_id=${ACCESS_KEY_ID} \
  --from-literal=access_key_secret=${SECRET_ACCESS_KEY} \
  --from-literal=bucketnames=${BUCKET_NAME} \
  --from-literal=endpoint=https://${BUCKET_HOST}:${BUCKET_PORT}</code></pre>
<h4>Create a LokiStack Instance</h4>
<pre><code class="language-plaintext">apiVersion: loki.grafana.com/v1
kind: LokiStack
metadata:
  name: logging-loki
  namespace: openshift-logging
spec:
  limits: 1
    global:
      retention: 2
        days: 20
        stream: 3
        - days: 4
          priority: 1
          selector: '{kubernetes_namespace_name=~"test.+"}'
  size: 1x.demo 4
  storage:
    secret:
      name: logging-loki-odf 5
      type: s3 6
    tls:
      caName: openshift-service-ca.crt 7
  storageClassName: ocs-external-storagecluster-ceph-rbd 8
  tenants:
    mode: openshift-logging</code></pre>
<figure class="table">
  <table style="background-color:rgb(255, 255, 255);">
    <tbody>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg" alt="1"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">Defines the limits that Loki applies to the log streams. See the references section for more information about the limit types for log streams.</td>
      </tr>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg" alt="2"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">This limit defines 20 days for log retention.</td>
      </tr>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg" alt="3"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">The retention policy that is based on log streams. Stream-based retention policies are useful if lowering the retention period for a log stream. For example, you might switch to debug mode temporarily in a project to troubleshoot a critical issue. However, debug logs can quickly fill your storage if the retention period is long. Then, you reduce the retention period for that project for a few days. The selector for the stream-based retention policy is specified in the LogQL query language, which is explained later in this section.</td>
      </tr>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/4.svg" alt="4"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">The size of the <code>LokiStack</code> instance. You can configure it by using sizes such as <code>1x.small</code> and <code>1x.medium</code>.</td>
      </tr>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/5.svg" alt="5"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">The name for the secret that contains the bucket credentials.</td>
      </tr>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/6.svg" alt="6"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">The corresponding storage type.</td>
      </tr>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/7.svg" alt="7"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">Loki must configure the Certificate Authority (CA) for the object storage endpoint if the object storage endpoint certificate is not trusted. For ODF, you can use the OpenShift service CA in the <code>openshift-service-ca.crt</code> configuration map. For other object stores, create a configuration map that contains the CA certificate.</td>
      </tr>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/8.svg" alt="8"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">The name of a storage class for temporary storage. You can list the storage classes for your cluster by using the <code>oc get storageclasses</code> command.</td>
      </tr>
    </tbody>
  </table>
</figure>
<h4>Configure the OpenShift Logging Log Store</h4>
<ul>
  <li>To configure OpenShift Logging to use Loki as the log store, you must create a cluster logging resource in the <code>openshift-logging</code> namespace and set <code>lokistack</code> as the log store type. Then, OpenShift Logging uses Loki to store the<mark class="marker-yellow"> infrastructure and application logs.</mark> The Logging operator does not include audit logs, because these logs might contain sensitive security details.</li>
</ul>
<pre><code class="language-plaintext">apiVersion: logging.openshift.io/v1
kind: ClusterLogging
metadata:
  name: instance
  namespace: openshift-logging
spec:
  managementState: Managed
  logStore:
    type: lokistack 1
    lokistack:
      name: logging-loki 2
  collection:
    type: vector</code></pre>
<figure class="table">
  <table style="background-color:rgb(255, 255, 255);">
    <tbody>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg" alt="1"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">The log store type.</td>
      </tr>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg" alt="2"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">The name for the internal <code>LokiStack</code> instance.</td>
      </tr>
    </tbody>
  </table>
</figure>
<ul>
  <li>To include<mark class="marker-yellow"> audit logs</mark> in the log store, you must create a <code>ClusterLogForwarder</code> CR to configure the log collector to collect them.</li>
</ul>
<pre><code class="language-plaintext">apiVersion: logging.openshift.io/v1
kind: ClusterLogForwarder
metadata:
  name: instance
  namespace: openshift-logging
spec:
  pipelines:
  - name: all-to-default
    inputRefs: 1
    - infrastructure
    - application
    - audit
    outputRefs: 2
    - default</code></pre>
<figure class="table">
  <table style="background-color:rgb(255, 255, 255);">
    <tbody>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg" alt="1"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">The type of logs to collect, which include infrastructure, audit, and application logs.</td>
      </tr>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg" alt="2"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">The destination for the logs, with the internal Loki instance being the default output.</td>
      </tr>
    </tbody>
  </table>
</figure>
<ul>
  <li>create a cluster logging resource in the <code>openshift-logging</code> namespace and set <code>ocp-console</code> as the visualization type.&nbsp;</li>
</ul>
<pre><code class="language-plaintext">apiVersion: logging.openshift.io/v1
kind: ClusterLogging
metadata:
  name: instance
  namespace: openshift-logging
spec:
...output omitted...
  visualization:
    type: ocp-console</code></pre>
<h4>LogQL Queries</h4>
<ul>
  <li>Loki organizes logs into streams that are called <i>chunks</i>.&nbsp;</li>
  <li>A <i>chunk</i> is a sequence of log entries that use the same set of labels.</li>
  <li>Labels are key-value pairs that help identify and filter log entries.</li>
  <li>A LogQL query consists of the following parameters:<ul>
      <li>The log stream selector, which accepts the following operators: <code>=</code> equals, <code>!=</code> not equals, <code>=~</code> regex pattern matches, and <code>!~</code> regex pattern does not match.</li>
      <li>A filter expression, which accepts the following operators: <code>|=</code> equals, <code>!=</code> not equals, <code>|~</code> regex pattern matches, and <code>!~</code> regex pattern does not match.</li>
    </ul>
  </li>
</ul>
<pre><code class="language-plaintext">log_type="infrastructure" | json
log_type="audit" | json | file=~".*audit.log" (filter)
log_type="audit" | json |= "error" != "warning" |~ "status [45]03" (more than one filter)</code></pre>
<ul>
  <li>Adding the <code>| json</code> string to your query extracts all JSON properties as labels for ease of viewing.&nbsp;</li>
</ul>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
