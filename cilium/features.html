<!--
title: features
description: 
published: true
date: 2025-12-09T19:08:19.181Z
tags: 
editor: ckeditor
dateCreated: 2025-12-09T18:50:53.314Z
-->

<h2>IPAM</h2>
<ul>
  <li>Each node get a slice of the POD CIDR.</li>
  <li>IPAM modes:<ul>
      <li>kubernetes host scope: the allocation of node specific CIDR range is assigned by kubernetes control plane (kube-controller-manager). pass the following options in the manifest file:<ul>
          <li>--allocate-node-cidrs=true</li>
          <li>--cluster-cidr</li>
          <li>--node-cidr-mask-size</li>
          <li>cilium poll the API server to get the Node resource and search for spec.podCIDR range and use it to assign IPs for pods.</li>
          <li>The Kubernetes host-scope IPAM mode is enabled with <code>ipam: kubernetes.</code> in the values.yaml</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<pre><code class="language-plaintext">apiVersion: v1
kind: Node
metadata:
  annotations:
    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
    node.alpha.kubernetes.io/ttl: "0"
    volumes.kubernetes.io/controller-managed-attach-detach: "true"
  creationTimestamp: "2025-12-09T14:17:33Z"
  labels:
    beta.kubernetes.io/arch: amd64
    beta.kubernetes.io/os: linux
    kubernetes.io/arch: amd64
    kubernetes.io/hostname: k8s-worker01
    kubernetes.io/os: linux
  name: k8s-worker01
  resourceVersion: "32821"
  uid: b3b8a079-7110-472f-9df4-cf91cc8d8b56
spec:
  podCIDR: 10.244.1.0/24
  podCIDRs:
  - 10.244.1.0/24
</code></pre>
<ul>
  <li>&nbsp;<ul>
      <li>Custer Scope (default)<ul>
          <li>Cilium operator allocate POD CIDRs to each node, and cilium use them.</li>
          <li>enabled by ipam.mode: “cluster-pool” and &nbsp;ipam.mode.operator.</li>
          <li>CiliumNode object stores the podCIDRs to use</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<pre><code class="language-plaintext">apiVersion: cilium.io/v2
kind: CiliumNode
metadata:
  labels:
    beta.kubernetes.io/arch: amd64
    beta.kubernetes.io/os: linux
    kubernetes.io/arch: amd64
    kubernetes.io/hostname: k8s-master
    kubernetes.io/os: linux
    node-role.kubernetes.io/control-plane: ""
    node.kubernetes.io/exclude-from-external-load-balancers: ""
  name: k8s-master
  ownerReferences:
  - apiVersion: v1
    kind: Node
    name: k8s-master

spec:
  addresses:
  - ip: 10.200.130.134
    type: InternalIP
  - ip: 10.0.0.244
    type: CiliumInternalIP
  alibaba-cloud: {}
  azure: {}
  bootid: 5d16ffdf-11fb-4fd6-b762-a47fce3db6ac
  encryption: {}
  eni: {}
  health:
    ipv4: 10.0.0.148
  ingress: {}
  ipam:
    podCIDRs:
    - 10.0.0.0/24
    pools: {}
</code></pre>
<p>&nbsp;</p>
<pre><code class="language-plaintext">kubectl exec cilium-pod -n kube-system -- cilium-dbg status --all-addresses

KVStore:                Disabled
Kubernetes:             Ok         1.33 (v1.33.6) [linux/amd64]
Kubernetes APIs:        ["EndpointSliceOrEndpoint", "cilium/v2::CiliumCIDRGroup", "cilium/v2::CiliumClusterwideNetworkPolicy", "cilium/v2::CiliumEndpoint", "cilium/v2::CiliumNetworkPolicy", "cilium/v2::CiliumNode", "core/v1::Pods", "networking.k8s.io/v1::NetworkPolicy"]
KubeProxyReplacement:   False
Host firewall:          Disabled
SRv6:                   Disabled
CNI Chaining:           none
CNI Config file:        successfully wrote CNI configuration file to /host/etc/cni/net.d/05-cilium.conflist
Cilium:                 Ok   1.18.4 (v1.18.4-afda2aa9)
NodeMonitor:            Listening for events on 2 CPUs with 64x4096 of shared memory
Cilium health daemon:   Ok
IPAM:                   IPv4: 7/254 allocated from 10.0.1.0/24,
Allocated addresses:
  10.0.1.118 (cilium-test-1/client3-795488bf5-hbnv7)
  10.0.1.126 (kube-system/hubble-relay-848bdc8547-gwrvs)
  10.0.1.181 (health)
  10.0.1.220 (router)
  10.0.1.243 (cilium-test-1/echo-other-node-6c6fdf86b5-nwk6z)
  10.0.1.51 (cilium-test-ccnp1/client-ccnp-df44777df-qddrp)
  10.0.1.62 (default/nginx)
IPv4 BIG TCP:            Disabled
IPv6 BIG TCP:            Disabled
BandwidthManager:        Disabled
Routing:                 Network: Tunnel [vxlan]   Host: Legacy
Attach Mode:             TCX
Device Mode:             veth
Masquerading:            IPTables [IPv4: Enabled, IPv6: Disabled]
Controller Status:       42/42 healthy
Proxy Status:            OK, ip 10.0.1.220, 0 redirects active on ports 10000-20000, Envoy: external
Global Identity Range:   min 256, max 65535
Hubble:                  Ok              Current/Max Flows: 4095/4095 (100.00%), Flows/s: 7.32   Metrics: Disabled
Encryption:              Disabled
Cluster health:          3/3 reachable   (2025-12-09T19:01:33Z)   (Probe interval: 1m56.754608943s)
Name                     IP              Node                     Endpoints
Modules Health:          Stopped(13) Degraded(0) OK(74)
</code></pre>
<ul>
  <li>&nbsp;<ul>
      <li>cloud-specific (aws,azure)</li>
      <li>multi-pool</li>
      <li>CRD backed</li>
    </ul>
  </li>
</ul>
