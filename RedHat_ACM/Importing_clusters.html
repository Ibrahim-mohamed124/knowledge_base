<!--
title: Chapter two ~ Earth
description: 
published: true
date: 2025-10-17T16:48:17.219Z
tags: 
editor: ckeditor
dateCreated: 2025-10-16T19:32:45.334Z
-->

<h3>Red&nbsp;Hat Advanced Cluster Management Lifecycle</h3>
<ul>
  <li>The <strong>multicluster engine operator </strong>uses the <strong>Hive operator</strong> that is provided with Red&nbsp;Hat OpenShift Container Platform (RHOCP) to provision clusters for all <mark class="marker-yellow">providers except the on-premise clusters and hosted control planes.</mark></li>
  <li>on-premise clusters, multicluster engine operator uses the<strong> central infrastructure management and the assisted installer function from RHOCP.</strong></li>
  <li>The <strong>Hive Operator</strong> is a Kubernetes Operator developed by Red Hat to automate the provisioning, management, and lifecycle of OpenShift clusters—especially in large-scale, multi-cluster environments.</li>
  <li>The RHACM installation process deploys <strong>the console as a plug-in to the hub OpenShift web console</strong>.</li>
  <li>Cloud Native Computing Foundation (CNCF) Kubernetes Conformance Program: ensures that Kubernetes implementations from different vendors meet a consistent set of standards, <strong>enabling interoperability</strong>, reliability, and portability across platforms.<ul>
      <li>Amazon Web Services</li>
      <li>Amazon Web Services GovCloud</li>
      <li>Microsoft Azure</li>
      <li>Google Cloud Platform</li>
      <li>VMware vSphere</li>
      <li>Red&nbsp;Hat OpenStack Platform</li>
      <li>On-premise</li>
    </ul>
  </li>
</ul>
<h2>Web console</h2>
<h3>Infrastructure tab</h3>
<figure class="table" style="width:1170px;">
  <table style="background-color:rgb(255, 255, 255);">
    <tbody>
      <tr>
        <td style="border-top:1px solid rgb(221, 221, 221);padding:8px;vertical-align:top;">Clusters</td>
        <td style="border-top:1px solid rgb(221, 221, 221);padding:8px;vertical-align:top;">Use to create clusters or to import existing clusters.</td>
      </tr>
      <tr>
        <td style="border-top:1px solid rgb(221, 221, 221);padding:8px;vertical-align:top;">Automation</td>
        <td style="border-top:1px solid rgb(221, 221, 221);padding:8px;vertical-align:top;">Use to create an Ansible template. You can create <strong>prehook and posthook Ansible job instances&nbsp;</strong><br>that occur before or after creating or upgrading your clusters.</td>
      </tr>
      <tr>
        <td style="border-top:1px solid rgb(221, 221, 221);padding:8px;vertical-align:top;">Host Inventory</td>
        <td style="border-top:1px solid rgb(221, 221, 221);padding:8px;vertical-align:top;">Use to create a host inventory to discover physical or virtual machines that you can install your RHOCP clusters on.</td>
      </tr>
      <tr>
        <td style="border-top:1px solid rgb(221, 221, 221);padding:8px;vertical-align:top;">Virtual Machines</td>
        <td style="border-top:1px solid rgb(221, 221, 221);padding:8px;vertical-align:top;">Use to manage virtual machines across all clusters.</td>
      </tr>
    </tbody>
  </table>
</figure>
<p>&nbsp;</p>
<h4>Creating clusters</h4>
<ol>
  <li>Create <code>ClusterDeployment</code> Custom Resource (CR): Core <mark class="marker-yellow">Hive resource</mark> to control the lifecycle of a cluster and the Hive API entry point</li>
  <li>The <code>ClusterDeployment</code> CR references the short name of a <code>ClusterImageSet</code> CR to specify<mark class="marker-green"> <strong>the RHOCP version to the new cluster.</strong></mark></li>
  <li>The <code>ClusterImageSet</code> CR: A CRD that is provided by the Hive operator to define the RHOCP installer images. That custom resource can be pointed at by ClusterDeployment resource to create new clusters on <strong>different platforms or provisioning workflows(agent-based, image-based(using images to install ocp)</strong>, ensuring <strong>version consistency</strong>.<ul>
      <li><strong>Immutable reference: </strong>The CRD reference installer images by digest, so it ensures that the same installer image version is used always</li>
      <li><strong>Decoupling installer from CRs: </strong>Upgrades and deploying new minor versions involve creating a <code>ClusterImageSet</code> CR, not editing every <code>ClusterDeployment</code> CR.</li>
      <li><strong>Central catalog of images: </strong>Administrators maintain a library of approved RHOCP installer images. Select the latest-approved or stable-4.x.x <code>ClusterImageSet</code> CR to avoid manually searching through quay.io or registry.redhat.io.</li>
    </ul>
  </li>
</ol>
<blockquote>
  <p>An <strong>RPM-hosted installer image</strong> is a pre-built container image that contains:</p>
  <p>The <strong>OpenShift installer binary</strong></p>
  <p>All required <strong>RPM packages</strong> for bootstrapping and configuring the cluster</p>
  <p>Dependencies for the <strong>Agent-based Installer</strong> or <strong>Zero Touch Provisioning (ZTP)</strong></p>
</blockquote>
<pre><code class="language-plaintext">apiVersion: hive.openshift.io/v1
kind: ClusterImageSet
metadata:
  labels:
    channel: fast
    visible: 'true'
  name: &lt;referenced by ClusterDeployment&gt;
spec:
  releaseImage: quay.io/openshift-release-dev/ocp-release:4.x.1-x86_64 2 &gt; version of ocp installer </code></pre>
<pre><code class="language-plaintext">apiVersion: hive.openshift.io/v1
kind: ClusterDeployment
metadata:
  name: mycluster
  namespace: mynamespace
spec:
  baseDomain: hive.example.com
  clusterName: mycluster
  platform:
    aws:
      credentialsSecretRef:
        name: mycluster-aws-creds
      region: us-east-1
  provisioning:
    imageSetRef:
      name: &lt;name of clusterImageSet&gt;
    installConfigSecretRef:
      name: mycluster-install-config
    sshPrivateKeySecretRef:
      name: mycluster-ssh-key
  pullSecretRef:
    name: mycluster-pull-secret</code></pre>
<p>&nbsp;</p>
<p><strong>&nbsp;4. </strong>The Hive controller watches the <code>imageSetRef.name</code> field, looks up the corresponding <code>ClusterImageSet</code> CR, and uses its <code>spec.releaseImage</code> to provision the cluster.</p>
<h4><strong>Upgrading a cluster</strong></h4>
<ol>
  <li>Just change the imageSetRef.name to a later one with higher version</li>
</ol>
<h4>Importing Existing Clusters</h4>
<p>From the RHACM web console, go to the <strong>Infrastructure</strong> → <strong>Clusters</strong> menu.</p>
<p>Click <strong>Import cluster</strong>.</p>
<p>Provide the required import cluster name.</p>
<p>Optionally select a cluster set for the import. The import uses the <code>default</code> managed cluster set if nothing is selected.</p>
<p>Optionally add labels. If you remove a hub cluster and try to import it again, you must add the <code>local-cluster:true</code> label to the <code>ManagedCluster</code> CR.</p>
<p>For the <code>Import mode</code>, use <code>Run import command manually</code>.</p>
<p>Click <strong>Next</strong>.</p>
<p>Click <strong>Next</strong> on the <code>Automation</code> page.</p>
<p>Click <strong>Generate command</strong> on the <code>Review</code> page.</p>
<p>Click <strong>Copy command</strong> to copy the generated command and token to the clipboard.</p>
<p>Paste and run the copied command in a terminal session on the target cluster.</p>
<p>Return to the <strong>Infrastructure</strong> → <strong>Clusters</strong> menu.</p>
<p>The newly imported cluster is immediately displayed in the <strong>Infrastructure</strong> → <strong>Clusters</strong> menu. It takes several minutes to see all the cluster details, because RHACM is installing the add-ons in the managed cluster.</p>
<ul>
  <li>import modes:<ul>
      <li>Run import command manually on the target cluster</li>
      <li>enter the API url and API token for the target cluster</li>
      <li>KubeConfig certs</li>
    </ul>
  </li>
</ul>
<h4>Scaling a Cluster</h4>
<ol>
  <li><code>SiteConfig</code> operator: Red Hat OpenShift component that automates cluster provisioning and configuration—<strong>especially for edge and single-node deployments</strong>—using declarative YAML resources.<ul>
      <li>It introduces a <strong>ClusterInstance API</strong>, which <mark class="marker-yellow">separates cluster definitions from installation methods</mark>, making it easier to manage clusters declaratively.</li>
      <li>A template-driven cluster provisioning solution with the <code>ClusterInstance</code> API</li>
      <li>Dynamically generates installation manifests based on user-defined templates that are instantiated from the data in the <code>ClusterInstance</code> CR.</li>
      <li>overview:<ol>
          <li>Create one or more sets of <mark class="marker-yellow">installation templates </mark>that are used on the hub cluster</li>
          <li>Create a <code>ClusterInstance</code> CR that<mark class="marker-yellow"> references those installation templates</mark> and supporting manifests.</li>
          <li>After the resources are created, the <code>SiteConfig</code> operator reconciles the <code>ClusterInstance</code> CR by <mark class="marker-yellow">populating the template fields that are referenced in the CR.</mark></li>
          <li>The <code>SiteConfig</code> operator validates and renders the installation manifests, and then the operator performs a dry run.</li>
          <li>If the dry run is successful, then the manifests are created, and then the <mark class="marker-yellow">underlying operators consume and process the manifests to start the instillation.</mark></li>
          <li>The <code>SiteConfig</code> operator <mark class="marker-pink">continuously monitors</mark> for changes in the associated <code>ClusterDeployment</code> CR and updates the <code>ClusterInstance</code> <strong>status field accordingly.</strong></li>
        </ol>
      </li>
    </ul>
  </li>
</ol>
<blockquote>
  <p>To scale the cluster, set the <code>clusterinstance.spec.nodes</code> list to use the appropriate number of nodes for the installation.</p>
</blockquote>
<blockquote>
  <p><strong>The Hive Operator and SiteConfig Operator both automate OpenShift cluster provisioning, but they serve different use cases: Hive is designed for dynamic, cloud-based Cluster-as-a-Service models, while SiteConfig is tailored for declarative, GitOps-driven edge deployments using image-based workflows.</strong></p>
</blockquote>
<figure class="table">
  <table>
    <thead>
      <tr>
        <th>Feature</th>
        <th><strong>Hive Operator</strong></th>
        <th><strong>SiteConfig Operator</strong></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Primary Use Case</strong></td>
        <td>Cluster-as-a-Service (CaaS) for dynamic provisioning</td>
        <td>Declarative provisioning for edge and disconnected clusters</td>
      </tr>
      <tr>
        <td><strong>Deployment Style</strong></td>
        <td>API-driven, dynamic installs</td>
        <td>GitOps-driven, image-based installs</td>
      </tr>
      <tr>
        <td><strong>Target Environments</strong></td>
        <td>Cloud platforms (AWS, Azure, GCP), bare metal</td>
        <td>Far-edge, Single Node OpenShift (SNO), disconnected sites</td>
      </tr>
      <tr>
        <td><strong>Installation Method</strong></td>
        <td>Uses OpenShift installer and dynamic provisioning</td>
        <td>Uses prebuilt RPM-hosted installer images</td>
      </tr>
      <tr>
        <td><strong>Key Resources</strong></td>
        <td><code>ClusterDeployment</code>, <code>ClusterPool</code>, <code>SyncSet</code></td>
        <td><code>SiteConfig</code>, <code>ClusterInstance</code>, <code>BareMetalHost</code></td>
      </tr>
      <tr>
        <td><strong>Integration</strong></td>
        <td>Works with RHACM for cluster lifecycle management</td>
        <td>Works with ZTP and GitOps pipelines</td>
      </tr>
      <tr>
        <td><strong>Customization</strong></td>
        <td>Post-install via SyncSets and MachinePools</td>
        <td>Pre-install via declarative YAML and GitOps</td>
      </tr>
      <tr>
        <td><strong>Provisioning Speed</strong></td>
        <td>Slower (dynamic install per request)</td>
        <td>Faster (prebuilt image-based install)</td>
      </tr>
      <tr>
        <td><strong>Disconnected Support</strong></td>
        <td>Limited</td>
        <td>Full support</td>
      </tr>
    </tbody>
  </table>
</figure>
<blockquote>
  <figure class="table">
    <table>
      <tbody>
        <tr>
          <td><strong>SiteConfig Operator</strong></td>
          <td>Reads <code>SiteConfig</code>, generates Hive resources, monitors status</td>
        </tr>
        <tr>
          <td><strong>Hive Operator</strong></td>
          <td>Implements <code>ClusterDeployment</code>, provisions clusters, manages lifecycle</td>
        </tr>
      </tbody>
    </table>
  </figure>
</blockquote>
<h4>Removing Imported Clusters from RHACM</h4>
<ol>
  <li>Detaching the cluster will automatically remove most of RHACM components on the managed clusters</li>
  <li>Some components should be removed manually:</li>
</ol>
<ul>
  <li>The <code>klusterlet</code> cluster role</li>
  <li>The <code>open-cluster-management: klusterlet-admin-aggregate-clusterrole</code> cluster role</li>
  <li>The <code>klusterlet</code> cluster role binding</li>
</ul>
<h3>Home tab</h3>
<p>The <strong>Home</strong> section provides information about RHACM and its use cases, and includes shortcuts to the main product features. The section contains two submenus:</p>
<ul>
  <li><strong>Welcome:&nbsp;</strong> provides information and links to access the main RHACM features. This page includes a link for contacting the Red&nbsp;Hat support team.</li>
  <li><strong>Overview: </strong>provides a summary and a high-level overview of the details and statuses of the managed clusters.</li>
</ul>
<h3>Applications tab</h3>
<ul>
  <li>Use the <strong>Applications</strong> section to create, deploy, or manage applications across a cluster fleet. For this component, RHACM integrates with Red&nbsp;Hat OpenShift GitOps, which is based on Argo CD.</li>
</ul>
<h3>Governance tab</h3>
<ul>
  <li>Use the <strong>Governance</strong> section to control and enforce compliance standards for the cluster fleet. You create and manage policies and policy controllers, and apply them to the cluster fleet.</li>
</ul>
<h3>Credentials tab</h3>
<ul>
  <li>A credential stores the connection details that RHACM uses for<mark class="marker-yellow"> accessing an external entity</mark>, such as a cloud provider or an external application platform. RHACM uses the following credential types:<ul>
      <li><strong>Cloud provider credentials,</strong> such as Amazon Web Services, Google Cloud Platform, and Microsoft Azure</li>
      <li><strong>Data center credentials</strong>, such as Red&nbsp;Hat OpenStack Platform and bare metal resources</li>
      <li>Automation and other credentials, such as<strong> Red&nbsp;Hat Ansible Automation Platform</strong></li>
      <li>Use the <strong>Credentials</strong> section to create and administer credentials for all your cloud providers and systems</li>
    </ul>
  </li>
</ul>
<h3>Search tab</h3>
<ul>
  <li>provides visibility of the Kubernetes objects across the cluster fleet from a single user interface.</li>
</ul>
<figure class="image image_resized" style="width:100%;"><img src="/search-architecture.svg"></figure>
<ul>
  <li>Collector: In the hub cluster, the collector is deployed in the <code>open-cluster-management</code> namespace. In the other managed clusters, the collector is deployed in the <code>open-cluster-management-agent-addon</code> namespace, as part of the <code>search-collector</code> add-on.<ul>
      <li>Uses the <mark class="marker-yellow">Kubernetes API</mark> to collect the information about the objects, and then the collector <mark class="marker-yellow">computes relationships for objects within the managed clusters.</mark></li>
    </ul>
  </li>
  <li>Indexer<strong>: </strong>Deployed only in the RHACM hub cluster, in the <code>open-cluster-management</code> namespace.&nbsp;<ul>
      <li>Receives data from the collectors and writes that data to a <mark class="marker-yellow">PostgreSQL database that runs as a pod in the hub cluster</mark></li>
      <li>The search filters are the keys of the indexing process &gt; the filter name is the key and values are the objects values of a specific attribute &nbsp;e.g metadata.namespace(indexer key): value</li>
      <li>&nbsp;Not all of objects attributes are indexed</li>
    </ul>
  </li>
  <li>Search API: The search API uses the RBAC of each managed cluster. For example, <mark class="marker-yellow">you can search for objects in a managed cluster only where you have authorization</mark>.<ul>
      <li>Some filters<mark class="marker-yellow"> allow arithmetic comparators for numeric fields</mark>, such as the <code>cpu</code>, <code>replicas</code>, <code>capacity</code>, and <code>memory</code> filters.</li>
    </ul>
  </li>
</ul>
<h4>Configuring RHACM Search</h4>
<h5>Persistent Storage for the Database</h5>
<ul>
  <li>By default PostgreSQL stores data in empty directory volume.</li>
  <li>To configure PVC: Edit the &nbsp;<code>search-v2-operator</code> resource of the <code>Search</code> kind in the <code>open-cluster-management</code> namespace</li>
</ul>
<pre><code class="language-plaintext">apiVersion: search.open-cluster-management.io/v1alpha1
kind: Search
metadata:
  name: search-v2-operator
  namespace: open-cluster-management
spec:
  dbStorage:
    size: 10Gi
    storageClassName: &lt;SC name&gt;</code></pre>
<h5>Resource Allocations</h5>
<ul>
  <li>Set the limits in the <code>search-v2-operator</code> resource in the <code>open-cluster-management</code> namespace in the hub cluster.</li>
</ul>
<pre><code class="language-plaintext">apiVersion: search.open-cluster-management.io/v1alpha1
kind: Search
metadata:
  name: search-v2-operator
  namespace: open-cluster-management
spec:
  dbStorage:
    size: 10Gi
    storageClassName: fastdisk
  deployments:
    collector:
      resources:
        limits:
          cpu: 500m
          memory: 128Mi
        requests:
          cpu: 250m
          memory: 64Mi
    database: {}
    indexer:
      resources:
        limits:
          memory: 5Gi
        requests:
          memory: 1Gi
    queryapi: {}
...output omitted...</code></pre>
<ul>
  <li>Set the memory limits by annotating the <code>search-collector</code> resource of the <code>ManagedClusterAddOn</code> kind in the managed cluster namespace in the hub cluster.&nbsp;</li>
</ul>
<pre><code class="language-plaintext"> oc annotate ManagedClusterAddOn search-collector -n mycluster \
  addon.open-cluster-management.io/search_memory_request=512Mi \
  addon.open-cluster-management.io/search_memory_limit=1Gi</code></pre>
<h5>Saved Search Queries and Search Templates</h5>
<ul>
  <li>You can adjust the &nbsp;number of saved searches for each user by adding the <code>SAVED_SEARCH_LIMIT</code> key to the <code>console-mce-config</code> configuration map in the <code>multicluster-engine</code> namespace</li>
</ul>
<pre><code class="language-plaintext">apiVersion: v1
kind: ConfigMap
metadata:
  name: console-mce-config
  namespace: multicluster-engine
  annotations:
    installer.multicluster.openshift.io/release-version: 2.8.0
  labels:
    backplaneconfig.name: multiclusterengine
data:
  SAVED_SEARCH_LIMIT: "20"
  LOG_LEVEL: info
  ansibleIntegration: disabled
  awsPrivateWizardStep: enabled
  singleNodeOpenshift: disabled</code></pre>
<ul>
  <li>Replace the default search templates by creating the <code>console-search-config</code> configuration map in the <code>open-cluster-management</code> namespace.</li>
</ul>
<pre><code class="language-plaintext">kind: ConfigMap
apiVersion: v1
metadata:
  name: console-search-config
  namespace: open-cluster-management
data:
  suggestedSearches: |-
    [
      {
        "id": "search.suggested.&lt;uniqe_id&gt;.name",
        "name": "Pods in mycluster",
        "description": "Show the pods in the mycluster OCP cluster",
        "searchText": "kind:Pod cluster:mycluster"
      },
      {
        "id": "search.suggested.&lt;uniqe_id&gt;.name",
        "name": "Frontend deployments",
        "description": "Show deployments with the tier=frontend label",
        "searchText": "kind:Deployment label:tier=frontend"
      }
    ]</code></pre>
<p>&nbsp;</p>
<h2>Access Control</h2>
<ul>
  <li><i>managed cluster set: a group of managed clusters represented by <mark class="marker-yellow">ManagedClusterSet resource</mark>, which helps in configuring access to managed clusters</i>
    <ul>
      <li><code>ManagedClusterSetBinding</code> : Is a resource to bind manage cluster set to a <strong>namespace. </strong><mark class="marker-yellow">Granting the hub cluster users permissions to that namespace will grant them permissions to the managed clusters</mark></li>
      <li>Each cluster must be a member of a managed cluster set. A managed cluster <mark class="marker-yellow">cannot be included in more than one managed cluster set</mark>. It is <strong>not bound to any namespace</strong>, so it’s not used for RBAC.</li>
      <li>At instillation RHACM creates a default ManagedClusterSet that has cluster that<mark class="marker-yellow"> do not belong to other sets </mark>(cannot be deleted or updated)</li>
      <li>RHACM automatically creates a managed cluster set named <code>global</code>, and the namespace <code>open-cluster-management-global-set</code>. RHACM also creates a <code>ManagedClusterSetBinding</code> resource named <code>global</code> that binds the <code>global</code> managed cluster set to the <code>open-cluster-management-global-set</code> namespace.</li>
      <li>All created managed clusters belong to the global managed cluster set</li>
      <li>A managed cluster set labels all the clusters that belong to it with the <code>cluster.open-cluster-management.io/clusterset=<i>managed_clusterset_name</i></code> label. <mark class="marker-yellow">RHACM uses this label to identify the clusters that belong to the managed cluster set.</mark></li>
    </ul>
  </li>
</ul>
<h3>Role Definitions</h3>
<blockquote>
  <p><code>open-cluster-management</code> resources: are the resources that RHACM consumes to manage clusters<br>&nbsp; &nbsp; &nbsp; ManagedCluster, ManagedClusterSet, ManagedClusterSetBinding……….</p>
</blockquote>
<ul>
  <li>Openshift default roles:<ul>
      <li>Cluster-Admin: with cluster role binding &gt; superuser</li>
      <li>edit, view, admin: namespaced or cluster wide access to the open-cluster-management resources || <mark class="marker-yellow">administration?????? of the hub cluster???</mark></li>
    </ul>
  </li>
  <li>RHACM roles:<ul>
      <li><code><strong>open-cluster-management:cluster-manager-admin:</strong></code> RHACM superuser, can create ManagedCluster(import, create clusters)</li>
      <li><code><strong>open-cluster-management:admin:</strong><i><strong>managed_cluster_name:</strong></i></code> Admin access to specific ManagedCluster resource, created for users who created the ManagedCluster automatically.</li>
      <li><code><strong>open-cluster-management:view:</strong><i><strong>managed_cluster_name:</strong></i></code> &nbsp;RHACM view access to the <code>ManagedCluster</code> resource named <code><i>managed_cluster_name</i></code>.</li>
      <li><code><strong>open-cluster-management:managedclusterset:admin:</strong><i><strong>managed_clusterset_name:&nbsp;</strong></i></code> administrator access to the <code>managedcluster</code>, <code>clusterclaim</code>, <code>clusterdeployment</code>, and <code>clusterpool</code> resources, which have the <code>cluster.open-cluster-management.io/clusterset=<i>managed_clusterset_name</i></code> label.&nbsp;</li>
      <li><code><strong>open-cluster-management:managedclusterset:view:</strong><i><strong>managed_clusterset_name:</strong></i></code> RHACM view access to the <code>ManagedClusterSet</code> resource named <code><i>managed_clusterset_name</i></code></li>
      <li><code><strong>open-cluster-management:managedclusterset:bind:</strong><i><strong>managed_clusterset_name:</strong></i></code> The user can bind the <code>ManagedClusterSet</code> resource named <code><i>managed_clusterset_name</i></code> to a namespace. To give access to the managed clusters<ul>
          <li>RHACM view access to the <code>ManagedClusterSet</code> resource named <code><i>managed_clusterset_name</i></code>.&nbsp;</li>
        </ul>
      </li>
      <li><code><strong>open-cluster-management:​subscription-admin:</strong></code> can create Git subscriptions that deploy Kubernetes resources that are specified in YAML files to multiple namespaces.</li>
    </ul>
  </li>
</ul>
<p style="text-align:center;">Superuser</p>
<p style="text-align:center;">managed cluster set admin &amp; <sub>binding-admin</sub></p>
<p style="text-align:center;">managed cluster admin</p>
<p style="text-align:center;">susbcription-admin</p>
<p style="text-align:center;">&nbsp;</p>
<h2>Placement</h2>
<ul>
  <li>Namespace-scoped resource that defines a rule to <mark class="marker-yellow">dynamically select a set of managed clusters</mark> in one or multiple managed cluster sets, which are bound to the placement namespace(the managed cluster sets).</li>
  <li>To create the <code>Placement</code> resource, you need the <mark class="marker-yellow">cluster administrator</mark> and the <mark class="marker-yellow">cluster set administrator </mark>roles.</li>
  <li><strong>Predicates: </strong>section in the <code>Placement</code> resource enables you to filter managed clusters from managed cluster sets by using the <code>labelSelector</code> and the <code>claimSelector</code> parameters.&nbsp;<ul>
      <li><code>numberOfClusters</code> parameter defines the number of clusters to select.</li>
    </ul>
  </li>
</ul>
<blockquote>
  <p><code>ClaimSelector</code><strong> is used to filter clusters based on labels attached to their </strong><code>ClusterClaims</code><strong>.</strong><br>A <strong>ClusterClaim</strong> is a <strong>cluster-scoped custom resource</strong> created on a <strong>managed cluster</strong>. These claims are collected by the <strong>registration agent</strong> and synced to the <strong>status of the corresponding </strong><code>ManagedCluster</code> on the hub cluster.</p>
</blockquote>
<pre><code class="language-plaintext">apiVersion: cluster.open-cluster-management.io/v1beta1
kind: Placement
metadata:
  name: placement1
  namespace: default
spec:
  numberOfClusters: 3 
  clusterSets: 
    - dev
    - stage
  predicates: 
    - requiredClusterSelector:
        labelSelector: 
          matchLabels:
            purpose: test
        claimSelector: 
          matchExpressions:
            - key: region.open-cluster-management.io
              operator: In
              values:
                - us-west-1</code></pre>
<ul>
  <li><strong>Taints and Tolerations:</strong>
    <ul>
      <li>The taints are properties of the <code>ManagedClusters</code> resource.</li>
      <li>The tolerations apply to the <code>Placement</code> resource</li>
    </ul>
  </li>
</ul>
<pre><code class="language-plaintext">apiVersion: cluster.open-cluster-management.io/v1
kind: ManagedCluster
metadata:
  name: cluster1
spec:
  hubAcceptsClient: true
  taints:
    - effect: NoSelect
      key: gpu
      value: "true"
      timeAdded: '2025-04-23T05:12:23Z'</code></pre>
<pre><code class="language-plaintext">apiVersion: cluster.open-cluster-management.io/v1beta1
kind: Placement
metadata:
  name: placement1
  namespace: ns1
spec:
  tolerations:
    - key: gpu
      value: "true"
      operator: Equal
      tolerationSeconds: 300 </code></pre>
<ul>
  <li><strong>Prioritizers:</strong>
    <ul>
      <li>Sort the clusters by prioritizer scores and select the top <code>n</code> clusters from that group by using the <code>prioritizerPolicy</code> section in the <code>Placement</code> resource.</li>
      <li>prioritizer policy:<ul>
          <li>mode: additive or exact, In <code>Additive</code> mode, any prioritizer that is not explicitly enumerated is enabled in its default configurations. In <code>Exact</code> mode, any prioritizer that is not explicitly enumerated is weighted as zero.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<pre><code class="language-plaintext">  apiVersion: cluster.open-cluster-management.io/v1beta1
kind: Placement
metadata:
  name: placement1
  namespace: ns1
spec:
  numberOfClusters: 1 1
  prioritizerPolicy: 2
    mode: Additive 3
    configurations: 4
      - scoreCoordinate: 5
          builtIn: ResourceAllocatableCPU 6
        weight: 2 7
      - scoreCoordinate:
          builtIn: ResourceAllocatableMemory
        weight: 2</code></pre>
<ul>
  <li><strong>Placement Decisions:&nbsp;</strong>
    <ul>
      <li>The <code>Placement</code> resource creates one ore more <code>PlacementDecision</code> resources in the same namespace with the <code>cluster.open-cluster-management.io/placement=<i>placement_name</i></code> label.&nbsp;</li>
      <li>Contains the managed clusters that the placement selected.</li>
    </ul>
  </li>
</ul>
<pre><code class="language-plaintext">apiVersion: cluster.open-cluster-management.io/v1beta1
kind: PlacementDecision
metadata:
  labels:
    cluster.open-cluster-management.io/placement: placement1 
  name: placement1-decision-1
  namespace: default
status:
  decisions: &lt;ordered according to the score&gt;
    - clusterName: cluster1
    - clusterName: cluster2
    - clusterName: cluster3</code></pre>
<h3>Centralized Identity Management Service</h3>
<ul>
  <li>Neither Kubernetes nor OpenShift currently provides a way to federate all the internal OAuth servers of each cluster. To solve this problem, you can configure OAuth to specify a identity provider such as LDAP for centrally managing the users who access the managed clusters.</li>
</ul>
<h2>Troubleshooting Common Import Issues for Managed Clusters</h2>
<ul>
  <li>The network between the new cluster and the hub cluster does not meet the requirements. Specifically, TCP ports 6443 and 443 in both directions are required. Also, both clusters must resolve the DNS names of the API endpoints.</li>
  <li>The API endpoint TLS certificate is not trusted. The clusters must trust the certificate of their counterparts. If you use a custom certificate authority (CA) to issue these certificates, then ensure that the CA certificate is part of the trusted CA bundle.</li>
  <li>The time between the clusters is not synchronized. Ensure that you configured the <code>chrony</code> time service in your clusters.</li>
  <li>The Kubernetes managed cluster cannot pull the RHACM images from the Red&nbsp;Hat registries. Kubernetes clusters require a secret with your RHOCP pull secret for retrieving the images from Red&nbsp;Hat.</li>
  <li>The <mark class="marker-yellow">import controller pods </mark>that are running in the hub cluster failed</li>
</ul>
<pre><code class="language-plaintext">oc get pods -n multicluster-engine \
  -l app=managedcluster-import-controller-v2
oc logs -n multicluster-engine \
  -l app=managedcluster-import-controller-v2</code></pre>
<ul>
  <li>The import secret resource was not created in the hub cluster. For each managed cluster, RHACM creates a namespace with the name of the cluster. In this namespace, RHACM creates the <code><i>cluster-name</i>-import</code> secret.</li>
</ul>
<pre><code class="language-plaintext">oc get secret mycluster-import -n mycluster
oc logs -n multicluster-engine \
  -l app=managedcluster-import-controller-v2 &gt;&gt; responsible for creating the secret</code></pre>
<ul>
  <li>The <code>klusterlet</code> resource on the managed cluster has a degraded status.<ul>
      <li>Verify the status of the <code>RegistrationDesiredDegraded</code> and <code>WorkDesiredDegraded</code> conditions in the <code>.status.conditions</code> section of the resource. Both conditions must be false</li>
    </ul>
  </li>
</ul>
<pre><code class="language-plaintext">oc get managedcluster mycluster \
  -o jsonpath-as-json='{.status.conditions}'
oc get klusterlets klusterlet \
  -o jsonpath-as-json='{.status.conditions}'
[
    [
...output omitted...
        {
            "lastTransitionTime": "2025-04-28T10:07:39Z",
            "message": "deployments replicas are desired: 1",
            "observedGeneration": 2,
            "reason": "DeploymentsFunctional",
            "status": "False",
            "type": "RegistrationDesiredDegraded"
        },
        {
            "lastTransitionTime": "2025-04-28T10:07:39Z",
            "message": "deployments replicas are desired: 1",
            "observedGeneration": 2,
            "reason": "DeploymentsFunctional",
            "status": "False",
            "type": "WorkDesiredDegraded"
        },
...output omitted...
    ]
]</code></pre>
<ul>
  <li>Importing a managed cluster that was previously attached to another RHACM hub might fail. This issue occurs when the managed cluster was not properly detached: some <mark class="marker-yellow">stale resources</mark>, such as the <code>klusterlet</code> resource, might still exist.<ul>
      <li>clean up the managed cluster by deleting the <code>klusterlet</code> resource, the <code>open-cluster-management-agent</code> and <code>open-cluster-management-agent-addon</code> namespaces, and the RHACM custom resource definitions (CRDs).</li>
      <li><a href="https://access.redhat.com/solutions/5950901">https://access.redhat.com/solutions/5950901</a></li>
    </ul>
  </li>
  <li>When the network between the managed cluster and the hub cluster is unstable, the status of the managed cluster might alternate between <code>Unknown</code> and <code>Ready</code>.<ul>
      <li>Because RHACM retries the connection five times, the total grace period is <code>leaseDurationSeconds x 5</code>. For example, if you set the <code>leaseDurationSeconds</code> parameter to 120, then the grace period is 10 minutes (120 x 5 seconds).</li>
    </ul>
  </li>
</ul>
<pre><code class="language-plaintext">apiVersion: cluster.open-cluster-management.io/v1
kind: ManagedCluster
metadata:
...output omitted...
  name: mycluster
spec:
  hubAcceptsClient: true
  leaseDurationSeconds: 60 &gt;&gt; default reporting period
  managedClusterClientConfigs:
  - caBundle: LS0t...Cg==
...output omitted...</code></pre>
<ul>
  <li><strong>Collecting Data and Opening Support Cases:</strong></li>
</ul>
<pre><code class="language-plaintext">oc adm must-gather \
  --image=registry.redhat.io/rhacm2/acm-must-gather-rhel9:v2.13 \
  --dest-dir=./must-gather-rhacm
oc get clusterversion \
  -o jsonpath='{.items[].spec.clusterID}{"\n"}'
date +"%m-%d-%Y-%H-%M-%S"
tar cvaf \
  must-gather-05-25-2025-12-45-36-2e52...9c5b.tar.gz \
  ./must-gather-rhacm</code></pre>
<p>&nbsp;</p>
