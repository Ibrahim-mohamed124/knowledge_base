<!--
title: Chapter two ~ Earth
description: 
published: true
date: 2025-10-17T12:27:41.289Z
tags: 
editor: ckeditor
dateCreated: 2025-10-16T19:32:45.334Z
-->

<h3>Red&nbsp;Hat Advanced Cluster Management Lifecycle</h3>
<ul>
  <li>The <strong>multicluster engine operator </strong>uses the <strong>Hive operator</strong> that is provided with Red&nbsp;Hat OpenShift Container Platform (RHOCP) to provision clusters for all <mark class="marker-yellow">providers except the on-premise clusters and hosted control planes.</mark></li>
  <li>on-premise clusters, multicluster engine operator uses the<strong> central infrastructure management and the assisted installer function from RHOCP.</strong></li>
  <li>The <strong>Hive Operator</strong> is a Kubernetes Operator developed by Red Hat to automate the provisioning, management, and lifecycle of OpenShift clusters—especially in large-scale, multi-cluster environments.</li>
  <li>The RHACM installation process deploys <strong>the console as a plug-in to the hub OpenShift web console</strong>.</li>
  <li>Cloud Native Computing Foundation (CNCF) Kubernetes Conformance Program: ensures that Kubernetes implementations from different vendors meet a consistent set of standards, <strong>enabling interoperability</strong>, reliability, and portability across platforms.<ul>
      <li>Amazon Web Services</li>
      <li>Amazon Web Services GovCloud</li>
      <li>Microsoft Azure</li>
      <li>Google Cloud Platform</li>
      <li>VMware vSphere</li>
      <li>Red&nbsp;Hat OpenStack Platform</li>
      <li>On-premise</li>
    </ul>
  </li>
</ul>
<h2>Web console</h2>
<h3>Infrastructure tab</h3>
<figure class="table" style="width:1170px;">
  <table style="background-color:rgb(255, 255, 255);">
    <tbody>
      <tr>
        <td style="border-top:1px solid rgb(221, 221, 221);padding:8px;vertical-align:top;">Clusters</td>
        <td style="border-top:1px solid rgb(221, 221, 221);padding:8px;vertical-align:top;">Use to create clusters or to import existing clusters.</td>
      </tr>
      <tr>
        <td style="border-top:1px solid rgb(221, 221, 221);padding:8px;vertical-align:top;">Automation</td>
        <td style="border-top:1px solid rgb(221, 221, 221);padding:8px;vertical-align:top;">Use to create an Ansible template. You can create <strong>prehook and posthook Ansible job instances&nbsp;</strong><br>that occur before or after creating or upgrading your clusters.</td>
      </tr>
      <tr>
        <td style="border-top:1px solid rgb(221, 221, 221);padding:8px;vertical-align:top;">Host Inventory</td>
        <td style="border-top:1px solid rgb(221, 221, 221);padding:8px;vertical-align:top;">Use to create a host inventory to discover physical or virtual machines that you can install your RHOCP clusters on.</td>
      </tr>
      <tr>
        <td style="border-top:1px solid rgb(221, 221, 221);padding:8px;vertical-align:top;">Virtual Machines</td>
        <td style="border-top:1px solid rgb(221, 221, 221);padding:8px;vertical-align:top;">Use to manage virtual machines across all clusters.</td>
      </tr>
    </tbody>
  </table>
</figure>
<p>&nbsp;</p>
<h4>Creating clusters</h4>
<ol>
  <li>Create <code>ClusterDeployment</code> Custom Resource (CR): Core <mark class="marker-yellow">Hive resource</mark> to control the lifecycle of a cluster and the Hive API entry point</li>
  <li>The <code>ClusterDeployment</code> CR references the short name of a <code>ClusterImageSet</code> CR to specify<mark class="marker-green"> <strong>the RHOCP version to the new cluster.</strong></mark></li>
  <li>The <code>ClusterImageSet</code> CR: A CRD that is provided by the Hive operator to define the RHOCP installer images. That custom resource can be pointed at by ClusterDeployment resource to create new clusters on <strong>different platforms or provisioning workflows(agent-based, image-based(using images to install ocp)</strong>, ensuring <strong>version consistency</strong>.<ul>
      <li><strong>Immutable reference: </strong>The CRD reference installer images by digest, so it ensures that the same installer image version is used always</li>
      <li><strong>Decoupling installer from CRs: </strong>Upgrades and deploying new minor versions involve creating a <code>ClusterImageSet</code> CR, not editing every <code>ClusterDeployment</code> CR.</li>
      <li><strong>Central catalog of images: </strong>Administrators maintain a library of approved RHOCP installer images. Select the latest-approved or stable-4.x.x <code>ClusterImageSet</code> CR to avoid manually searching through quay.io or registry.redhat.io.</li>
    </ul>
  </li>
</ol>
<blockquote>
  <p>An <strong>RPM-hosted installer image</strong> is a pre-built container image that contains:</p>
  <p>The <strong>OpenShift installer binary</strong></p>
  <p>All required <strong>RPM packages</strong> for bootstrapping and configuring the cluster</p>
  <p>Dependencies for the <strong>Agent-based Installer</strong> or <strong>Zero Touch Provisioning (ZTP)</strong></p>
</blockquote>
<pre><code class="language-plaintext">apiVersion: hive.openshift.io/v1
kind: ClusterImageSet
metadata:
  labels:
    channel: fast
    visible: 'true'
  name: &lt;referenced by ClusterDeployment&gt;
spec:
  releaseImage: quay.io/openshift-release-dev/ocp-release:4.x.1-x86_64 2 &gt; version of ocp installer </code></pre>
<pre><code class="language-plaintext">apiVersion: hive.openshift.io/v1
kind: ClusterDeployment
metadata:
  name: mycluster
  namespace: mynamespace
spec:
  baseDomain: hive.example.com
  clusterName: mycluster
  platform:
    aws:
      credentialsSecretRef:
        name: mycluster-aws-creds
      region: us-east-1
  provisioning:
    imageSetRef:
      name: &lt;name of clusterImageSet&gt;
    installConfigSecretRef:
      name: mycluster-install-config
    sshPrivateKeySecretRef:
      name: mycluster-ssh-key
  pullSecretRef:
    name: mycluster-pull-secret</code></pre>
<p>&nbsp;</p>
<p><strong>&nbsp;4. </strong>The Hive controller watches the <code>imageSetRef.name</code> field, looks up the corresponding <code>ClusterImageSet</code> CR, and uses its <code>spec.releaseImage</code> to provision the cluster.</p>
<h4><strong>Upgrading a cluster</strong></h4>
<ol>
  <li>Just change the imageSetRef.name to a later one with higher version</li>
</ol>
<h4>Importing Existing Clusters</h4>
<p>From the RHACM web console, go to the <strong>Infrastructure</strong> → <strong>Clusters</strong> menu.</p>
<p>Click <strong>Import cluster</strong>.</p>
<p>Provide the required import cluster name.</p>
<p>Optionally select a cluster set for the import. The import uses the <code>default</code> managed cluster set if nothing is selected.</p>
<p>Optionally add labels. If you remove a hub cluster and try to import it again, you must add the <code>local-cluster:true</code> label to the <code>ManagedCluster</code> CR.</p>
<p>For the <code>Import mode</code>, use <code>Run import command manually</code>.</p>
<p>Click <strong>Next</strong>.</p>
<p>Click <strong>Next</strong> on the <code>Automation</code> page.</p>
<p>Click <strong>Generate command</strong> on the <code>Review</code> page.</p>
<p>Click <strong>Copy command</strong> to copy the generated command and token to the clipboard.</p>
<p>Paste and run the copied command in a terminal session on the target cluster.</p>
<p>Return to the <strong>Infrastructure</strong> → <strong>Clusters</strong> menu.</p>
<p>The newly imported cluster is immediately displayed in the <strong>Infrastructure</strong> → <strong>Clusters</strong> menu. It takes several minutes to see all the cluster details, because RHACM is installing the add-ons in the managed cluster.</p>
<ul>
  <li>import modes:<ul>
      <li>Run import command manually on the target cluster</li>
      <li>enter the API url and API token for the target cluster</li>
      <li>KubeConfig certs</li>
    </ul>
  </li>
</ul>
<h4>Scaling a Cluster</h4>
<ol>
  <li><code>SiteConfig</code> operator: Red Hat OpenShift component that automates cluster provisioning and configuration—<strong>especially for edge and single-node deployments</strong>—using declarative YAML resources.<ul>
      <li>It introduces a <strong>ClusterInstance API</strong>, which <mark class="marker-yellow">separates cluster definitions from installation methods</mark>, making it easier to manage clusters declaratively.</li>
      <li>A template-driven cluster provisioning solution with the <code>ClusterInstance</code> API</li>
      <li>Dynamically generates installation manifests based on user-defined templates that are instantiated from the data in the <code>ClusterInstance</code> CR.</li>
      <li>overview:<ol>
          <li>Create one or more sets of <mark class="marker-yellow">installation templates </mark>that are used on the hub cluster</li>
          <li>Create a <code>ClusterInstance</code> CR that<mark class="marker-yellow"> references those installation templates</mark> and supporting manifests.</li>
          <li>After the resources are created, the <code>SiteConfig</code> operator reconciles the <code>ClusterInstance</code> CR by <mark class="marker-yellow">populating the template fields that are referenced in the CR.</mark></li>
          <li>The <code>SiteConfig</code> operator validates and renders the installation manifests, and then the operator performs a dry run.</li>
          <li>If the dry run is successful, then the manifests are created, and then the <mark class="marker-yellow">underlying operators consume and process the manifests to start the instillation.</mark></li>
          <li>The <code>SiteConfig</code> operator <mark class="marker-pink">continuously monitors</mark> for changes in the associated <code>ClusterDeployment</code> CR and updates the <code>ClusterInstance</code> <strong>status field accordingly.</strong></li>
        </ol>
      </li>
    </ul>
  </li>
</ol>
<blockquote>
  <p>To scale the cluster, set the <code>clusterinstance.spec.nodes</code> list to use the appropriate number of nodes for the installation.</p>
</blockquote>
<blockquote>
  <p><strong>The Hive Operator and SiteConfig Operator both automate OpenShift cluster provisioning, but they serve different use cases: Hive is designed for dynamic, cloud-based Cluster-as-a-Service models, while SiteConfig is tailored for declarative, GitOps-driven edge deployments using image-based workflows.</strong></p>
</blockquote>
<figure class="table">
  <table>
    <thead>
      <tr>
        <th>Feature</th>
        <th><strong>Hive Operator</strong></th>
        <th><strong>SiteConfig Operator</strong></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Primary Use Case</strong></td>
        <td>Cluster-as-a-Service (CaaS) for dynamic provisioning</td>
        <td>Declarative provisioning for edge and disconnected clusters</td>
      </tr>
      <tr>
        <td><strong>Deployment Style</strong></td>
        <td>API-driven, dynamic installs</td>
        <td>GitOps-driven, image-based installs</td>
      </tr>
      <tr>
        <td><strong>Target Environments</strong></td>
        <td>Cloud platforms (AWS, Azure, GCP), bare metal</td>
        <td>Far-edge, Single Node OpenShift (SNO), disconnected sites</td>
      </tr>
      <tr>
        <td><strong>Installation Method</strong></td>
        <td>Uses OpenShift installer and dynamic provisioning</td>
        <td>Uses prebuilt RPM-hosted installer images</td>
      </tr>
      <tr>
        <td><strong>Key Resources</strong></td>
        <td><code>ClusterDeployment</code>, <code>ClusterPool</code>, <code>SyncSet</code></td>
        <td><code>SiteConfig</code>, <code>ClusterInstance</code>, <code>BareMetalHost</code></td>
      </tr>
      <tr>
        <td><strong>Integration</strong></td>
        <td>Works with RHACM for cluster lifecycle management</td>
        <td>Works with ZTP and GitOps pipelines</td>
      </tr>
      <tr>
        <td><strong>Customization</strong></td>
        <td>Post-install via SyncSets and MachinePools</td>
        <td>Pre-install via declarative YAML and GitOps</td>
      </tr>
      <tr>
        <td><strong>Provisioning Speed</strong></td>
        <td>Slower (dynamic install per request)</td>
        <td>Faster (prebuilt image-based install)</td>
      </tr>
      <tr>
        <td><strong>Disconnected Support</strong></td>
        <td>Limited</td>
        <td>Full support</td>
      </tr>
    </tbody>
  </table>
</figure>
<blockquote>
  <figure class="table">
    <table>
      <tbody>
        <tr>
          <td><strong>SiteConfig Operator</strong></td>
          <td>Reads <code>SiteConfig</code>, generates Hive resources, monitors status</td>
        </tr>
        <tr>
          <td><strong>Hive Operator</strong></td>
          <td>Implements <code>ClusterDeployment</code>, provisions clusters, manages lifecycle</td>
        </tr>
      </tbody>
    </table>
  </figure>
</blockquote>
<h4>Removing Imported Clusters from RHACM</h4>
<ol>
  <li>Detaching the cluster will automatically remove most of RHACM components on the managed clusters</li>
  <li>Some components should be removed manually:</li>
</ol>
<ul>
  <li>The <code>klusterlet</code> cluster role</li>
  <li>The <code>open-cluster-management: klusterlet-admin-aggregate-clusterrole</code> cluster role</li>
  <li>The <code>klusterlet</code> cluster role binding</li>
</ul>
<h3>Home tab</h3>
<p>The <strong>Home</strong> section provides information about RHACM and its use cases, and includes shortcuts to the main product features. The section contains two submenus:</p>
<ul>
  <li><strong>Welcome:&nbsp;</strong> provides information and links to access the main RHACM features. This page includes a link for contacting the Red&nbsp;Hat support team.</li>
  <li><strong>Overview: </strong>provides a summary and a high-level overview of the details and statuses of the managed clusters.</li>
</ul>
<h3>Applications tab</h3>
<ul>
  <li>Use the <strong>Applications</strong> section to create, deploy, or manage applications across a cluster fleet. For this component, RHACM integrates with Red&nbsp;Hat OpenShift GitOps, which is based on Argo CD.</li>
</ul>
<h3>Governance tab</h3>
<ul>
  <li>Use the <strong>Governance</strong> section to control and enforce compliance standards for the cluster fleet. You create and manage policies and policy controllers, and apply them to the cluster fleet.</li>
</ul>
<h3>Credentials tab</h3>
<ul>
  <li>A credential stores the connection details that RHACM uses for<mark class="marker-yellow"> accessing an external entity</mark>, such as a cloud provider or an external application platform. RHACM uses the following credential types:<ul>
      <li><strong>Cloud provider credentials,</strong> such as Amazon Web Services, Google Cloud Platform, and Microsoft Azure</li>
      <li><strong>Data center credentials</strong>, such as Red&nbsp;Hat OpenStack Platform and bare metal resources</li>
      <li>Automation and other credentials, such as<strong> Red&nbsp;Hat Ansible Automation Platform</strong></li>
      <li>Use the <strong>Credentials</strong> section to create and administer credentials for all your cloud providers and systems</li>
    </ul>
  </li>
</ul>
<h3>Search tab</h3>
<ul>
  <li>provides visibility of the Kubernetes objects across the cluster fleet from a single user interface.</li>
</ul>
<figure class="image image_resized" style="width:100%;"><img src="/search-architecture.svg"></figure>
<ul>
  <li>Collector: In the hub cluster, the collector is deployed in the <code>open-cluster-management</code> namespace. In the other managed clusters, the collector is deployed in the <code>open-cluster-management-agent-addon</code> namespace, as part of the <code>search-collector</code> add-on.<ul>
      <li>Uses the <mark class="marker-yellow">Kubernetes API</mark> to collect the information about the objects, and then the collector <mark class="marker-yellow">computes relationships for objects within the managed clusters.</mark></li>
    </ul>
  </li>
  <li>Indexer<strong>: </strong>Deployed only in the RHACM hub cluster, in the <code>open-cluster-management</code> namespace.&nbsp;<ul>
      <li>Receives data from the collectors and writes that data to a <mark class="marker-yellow">PostgreSQL database that runs as a pod in the hub cluster</mark></li>
      <li>The search filters are the keys of the indexing process &gt; the filter name is the key and values are the objects values of a specific attribute &nbsp;e.g metadata.namespace(indexer key): value</li>
      <li>&nbsp;Not all of objects attributes are indexed</li>
    </ul>
  </li>
  <li>Search API: The search API uses the RBAC of each managed cluster. For example, <mark class="marker-yellow">you can search for objects in a managed cluster only where you have authorization</mark>.<ul>
      <li>Some filters<mark class="marker-yellow"> allow arithmetic comparators for numeric fields</mark>, such as the <code>cpu</code>, <code>replicas</code>, <code>capacity</code>, and <code>memory</code> filters.</li>
    </ul>
  </li>
</ul>
<h4>Configuring RHACM Search</h4>
<h5>Persistent Storage for the Database</h5>
<ul>
  <li>By default PostgreSQL stores data in empty directory volume.</li>
  <li>To configure PVC: Edit the &nbsp;<code>search-v2-operator</code> resource of the <code>Search</code> kind in the <code>open-cluster-management</code> namespace</li>
</ul>
<pre><code class="language-plaintext">apiVersion: search.open-cluster-management.io/v1alpha1
kind: Search
metadata:
  name: search-v2-operator
  namespace: open-cluster-management
spec:
  dbStorage:
    size: 10Gi
    storageClassName: &lt;SC name&gt;</code></pre>
<h5>Resource Allocations</h5>
<ul>
  <li>Set the limits in the <code>search-v2-operator</code> resource in the <code>open-cluster-management</code> namespace in the hub cluster.</li>
</ul>
<pre><code class="language-plaintext">apiVersion: search.open-cluster-management.io/v1alpha1
kind: Search
metadata:
  name: search-v2-operator
  namespace: open-cluster-management
spec:
  dbStorage:
    size: 10Gi
    storageClassName: fastdisk
  deployments:
    collector:
      resources:
        limits:
          cpu: 500m
          memory: 128Mi
        requests:
          cpu: 250m
          memory: 64Mi
    database: {}
    indexer:
      resources:
        limits:
          memory: 5Gi
        requests:
          memory: 1Gi
    queryapi: {}
...output omitted...</code></pre>
<ul>
  <li>Set the memory limits by annotating the <code>search-collector</code> resource of the <code>ManagedClusterAddOn</code> kind in the managed cluster namespace in the hub cluster.&nbsp;</li>
</ul>
<pre><code class="language-plaintext"> oc annotate ManagedClusterAddOn search-collector -n mycluster \
  addon.open-cluster-management.io/search_memory_request=512Mi \
  addon.open-cluster-management.io/search_memory_limit=1Gi</code></pre>
<h5>Saved Search Queries and Search Templates</h5>
<ul>
  <li>You can adjust the &nbsp;number of saved searches for each user by adding the <code>SAVED_SEARCH_LIMIT</code> key to the <code>console-mce-config</code> configuration map in the <code>multicluster-engine</code> namespace</li>
</ul>
<pre><code class="language-plaintext">apiVersion: v1
kind: ConfigMap
metadata:
  name: console-mce-config
  namespace: multicluster-engine
  annotations:
    installer.multicluster.openshift.io/release-version: 2.8.0
  labels:
    backplaneconfig.name: multiclusterengine
data:
  SAVED_SEARCH_LIMIT: "20"
  LOG_LEVEL: info
  ansibleIntegration: disabled
  awsPrivateWizardStep: enabled
  singleNodeOpenshift: disabled</code></pre>
<ul>
  <li>Replace the default search templates by creating the <code>console-search-config</code> configuration map in the <code>open-cluster-management</code> namespace.</li>
</ul>
<pre><code class="language-plaintext">kind: ConfigMap
apiVersion: v1
metadata:
  name: console-search-config
  namespace: open-cluster-management
data:
  suggestedSearches: |-
    [
      {
        "id": "search.suggested.&lt;uniqe_id&gt;.name",
        "name": "Pods in mycluster",
        "description": "Show the pods in the mycluster OCP cluster",
        "searchText": "kind:Pod cluster:mycluster"
      },
      {
        "id": "search.suggested.&lt;uniqe_id&gt;.name",
        "name": "Frontend deployments",
        "description": "Show deployments with the tier=frontend label",
        "searchText": "kind:Deployment label:tier=frontend"
      }
    ]</code></pre>
