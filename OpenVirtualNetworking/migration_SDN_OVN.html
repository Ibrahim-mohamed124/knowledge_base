<!--
title: Migration From SDN to OVN
description: 
published: true
date: 2026-01-13T12:12:27.469Z
tags: 
editor: ckeditor
dateCreated: 2026-01-13T08:53:05.970Z
-->

<h1>Methods of Migration</h1>
<ol>
  <li>Ansible palybooks for offline migration automation</li>
  <li>Offline Migration used for on-prem clusters</li>
  <li>Limited live migration to automatically migrate from SDN to OVN</li>
</ol>
<h3>Limited Live Migration</h3>
<ul>
  <li>No service interruption</li>
  <li>supported on vmware, baremetal, nutanix, and openstack</li>
</ul>
<blockquote>
  <p>there are multiple NICs inside of the host, and the default route is not on the interface that has the Kubernetes NodeIP, you must use the offline migration instead</p>
</blockquote>
<h3>Prerequisites from this <a href="https://access.redhat.com/solutions/7057169">article&nbsp;</a></h3>
<ul>
  <li><code>EgressIP</code> functionality is disabled during a limited live migration<ul>
      <li>Look for usage of <code>EgressIP</code> in your cluster by checking for <code>egressIPs</code> or <code>egressCIDRs</code> on your <code>NetNamespace</code> and <code>HostSubnet</code> objects</li>
    </ul>
  </li>
</ul>
<pre><code class="language-plaintext">oc get netnamespace -A | awk '$3 != ""'</code></pre>
<p>&nbsp;</p>
<pre><code class="language-plaintext">oc get hostsubnet | awk '$5 != ""'</code></pre>
<p>&nbsp;</p>
<blockquote>
  <p>EgressIP is a CRD allowing the user to define a fixed source IP for all egress traffic originating from any pods which match the EgressIP resource according to its spec definition.</p>
</blockquote>
<ul>
  <li>check for <strong>EgressNetworkPolicy</strong></li>
</ul>
<pre><code class="language-plaintext">oc get egressnetworkpolicies.network.openshift.io -A</code></pre>
<p>&nbsp;</p>
<ul>
  <li><strong>Egress Router</strong>
    <ul>
      <li>The <code>EgressRouter</code> functionality will not work during the limited live migration, is not automatically migrated, and is required to be reconfigured once the migration is complete, check the existence of Egress router pods</li>
      <li>Egresss router pods block the limited live migration process. <mark class="marker-yellow">They must be removed before beginning the limited live migration process.</mark></li>
    </ul>
  </li>
</ul>
<pre><code class="language-plaintext">oc get pods --all-namespaces -o json | jq '.items[] | select(.metadata.annotations."pod.network.openshift.io/assign-macvlan" == "true") | {name: .metadata.name, namespace: .metadata.namespace}'</code></pre>
<p>&nbsp;</p>
<ul>
  <li><strong>Multicast</strong>
    <ul>
      <li><code>Multicast</code> functionality is disabled during a limited live migration</li>
    </ul>
  </li>
</ul>
<pre><code class="language-plaintext">oc get netnamespace -o json | jq -r '.items[] | select(.metadata.annotations."netnamespace.network.openshift.io/multicast-enabled" == "true") | .metadata.name'</code></pre>
<p>&nbsp;</p>
<ul>
  <li><strong>Multitenant Isolation Mode</strong>
    <ul>
      <li>This feature is only supported by <code>OpenShift-SDN</code> and will stop working during and after the migration is completed</li>
      <li>Look for Multitenant Isolation Mode by checking the <code>Network</code> custom resource for the <code>mode: Multitenant</code> configuration:</li>
    </ul>
  </li>
  <li><strong>100.64.0.0/16 and 100.88.0.0/16 Address Range Usage</strong>
    <ul>
      <li>OVN-Kubernetes uses the <code>100.64.0.0/16</code> and <code>100.88.0.0/16</code> IP range internally by default and this can not overlap.</li>
    </ul>
  </li>
</ul>
<pre><code class="language-plaintext">oc get network/cluster -o json | jq '.spec | .clusterNetwork[].cidr,.serviceNetwork'</code></pre>
<p>&nbsp;</p>
<blockquote>
  <p>&nbsp;It is also recommended to check with your datacenter networking team to verify these range are not used anywhere else on external networks within your OpenShift infrastructure.</p>
</blockquote>
<ul>
  <li><strong>Maximum Transmission Unit - MTU</strong>
    <ul>
      <li>During the limited live migration, both OVN-Kubernetes and OpenShift SDN run in parallel. OVN-Kubernetes manages the cluster network of some nodes, while OpenShift SDN manages the cluster network of others. To ensure that cross-CNI traffic remains functional, the Cluster Network Operator updates the routable MTU to ensure that both CNIs share the same overlay MTU. As a result, after the migration has completed, the cluster MTU is 50 bytes less.</li>
    </ul>
  </li>
  <li><strong>NodeNetworkConfigurationPolicy</strong>
    <ul>
      <li>The OpenShiftSDN CNI plugin supports a NodeNetworkConfigurationPolicy (NNCP) custom resource (CR) to configure the primary interface on a node. The OVN-Kubernetes network plugin does not have this capability.</li>
    </ul>
  </li>
</ul>
<pre><code class="language-plaintext">oc get nncp</code></pre>
<p>&nbsp;</p>
<ul>
  <li><strong>Multiple Network Interface Cards on a Node</strong>
    <ul>
      <li>if these secondary interfaces were set up on the default network interface card (NIC) of the host, using the network types like MACVLAN, IPVLAN, SR-IOV, or bridge interfaces with the default NIC as the control node, OVN-Kubernetes might encounter issues.</li>
    </ul>
  </li>
</ul>
<pre><code class="language-plaintext">oc get network-attachment-definitions -A</code></pre>
<p>&nbsp;</p>
<ul>
  <li><strong>Static Routes and Routing Policies</strong>
    <ul>
      <li>If the cluster depends on static routes or routing policies via the host network so that pods can reach specific external destinations, you must set <code>routingViaHost</code> to <code>true</code> and <code>ipForwarding</code> to <code>Global</code> on the <code>gatewayConfig</code> key in the <code>network.operator.openshift.io</code> CR before you start the Limited Live Migration.</li>
    </ul>
  </li>
</ul>
<pre><code class="language-plaintext"> oc get network.operator.openshift.io cluster -o yaml</code></pre>
<p>&nbsp;</p>
<ul>
  <li><strong>Unmanaged DaemonSets</strong>
    <ul>
      <li>All DaemonSet objects in the <code>openshift-sdn</code> namespace, which are not managed by the Cluster Network Operator (CNO), must be removed before initiating the Limited Live Ligration.</li>
    </ul>
  </li>
</ul>
<pre><code class="language-plaintext">oc get daemonset -n openshift-sdn</code></pre>
<p>&nbsp;</p>
<ul>
  <li><strong>GitOps Configuration Management</strong>
    <ul>
      <li>If you are using a GitOps methodology to manage cluster configurations, specifically cluster CRs like <code>network.operator.openshift.io</code>, you need to disable the synchronization and management of these resources so they can be modified and managed by the Limited Live Migration and ensure the changes are not reverted.</li>
    </ul>
  </li>
  <li><strong>kube-proxy customization</strong>
    <ul>
      <li>The <code>OVNKubernetes</code> CNI does not support custom <code>kube-proxy</code> configurations. If you are customizing <code>kube-proxy</code> through the <code>kubeProxyConfig</code> key in the <code>network.operator.openshift.io</code> CR, you need to edit it and remove the options, you can perform this changes even after migration to OVN is completed.</li>
    </ul>
  </li>
  <li>the hardware time stamping cannot be applied to primary interface devices, such as an Open vSwitch (OVS) bridge. As a result, UDP version 4 configurations cannot work with a <code>br-ex</code> interface.</li>
  <li>if run an Operator or you have configured any application with the <mark class="marker-yellow">pod disruption budget,</mark> you might experience an interruption during the update process. If <code>minAvailable</code> is set to 1 in <code>PodDisruptionBudget</code>, the nodes are drained to apply pending machine configs which might block the eviction process. If several nodes are rebooted, all the pods might run on only one node, and the <code>PodDisruptionBudget</code> field can prevent the node drain</li>
  <li>to prevent traffic flow issues, check existing network policies in any namespaces that host applications that rely on system components. If a policy exists, enable traffic that originates from <code>openshift-ingress</code> or <code>openshift-kube-apiserver</code> system services to prevent the default setting from blocking this traffic.</li>
</ul>
<h3>workflow</h3>
<ol>
  <li>User initiate the migration by patching the default network CR</li>
</ol>
<pre><code class="language-plaintext">oc patch Network.config.openshift.io cluster --type='merge' --patch '{"metadata":{"annotations":{"network.openshift.io/network-type-migration":""}},"spec":{"networkType":"OVNKubernetes"}}'</code></pre>
<p>&nbsp; 2. The migration program:</p>
<ul>
  <li>Sets migration related fields in the <code>network.operator</code> custom resource (CR) and waits for routable MTUs to be applied to all nodes.</li>
  <li>Patches the <code>network.operator</code> CR to set the migration mode to <code>Live</code> for OVN-Kubernetes and deploys the OpenShift SDN network plugin in migration mode.</li>
  <li>Deploys OVN-Kubernetes with hybrid overlay enabled, ensuring that no racing conditions occur.</li>
  <li>Waits for the OVN-Kubernetes deployment and updates the conditions in the status of the <code>network.config</code> CR.</li>
  <li>Triggers the Machine Config Operator (MCO) to apply the new machine config to each machine config pool, which includes node cordoning, draining, and rebooting.</li>
  <li>OVN-Kubernetes adds nodes to the appropriate zones and recreates pods using OVN-Kubernetes as the default CNI plugin.</li>
  <li>Removes migration-related fields from the network.operator CR and performs cleanup actions, such as deleting OpenShift SDN resources and redeploying OVN-Kubernetes in normal mode with the necessary configurations.</li>
  <li>Waits for the OVN-Kubernetes redeployment and updates the status conditions in the <code>network.config</code> CR to indicate migration completion. If your migration is blocked, see "Checking limited live migration metrics" for information on troubleshooting the issue.</li>
</ul>
<h3>Monitoring the stages of the migration</h3>
<ul>
  <li>follow the progress of the migration using the following statuses set on the <code>network.config</code> CustomResource (CR).</li>
</ul>
<pre><code class="language-plaintext">oc get network.config.openshift.io cluster -o yaml</code></pre>
<p>&nbsp;</p>
