<!--
title: upscaling 
description: 
published: true
date: 2025-11-22T16:45:13.253Z
tags: 
editor: ckeditor
dateCreated: 2025-11-22T16:08:48.439Z
-->

<h1>S<sup>torage </sup>E<sup>xpansion</sup>&nbsp;</h1>
<p>ðŸ˜• Now, without autopilot, you have to expand the cluster by yourself, and there are two methods to do that.&nbsp;</p>
<ol>
  <li>Add new drives: All drives in the pool need to be resized to the same size for this operation.</li>
  <li>Resize existing drives: The new drive(s) will need to match the existing drives in size and IOPS properties (if available).<ul>
      <li>a significant amount of data movement to restriped the disks</li>
      <li>the pool runs in degraded mode during the <code>add-drive</code> operation.</li>
    </ul>
  </li>
</ol>
<blockquote>
  <ul>
    <li>Portworx does not support the <code>add-drive</code> operations with the PX-StoreV2 backend.</li>
  </ul>
</blockquote>
<h2><strong>Expand your storage pool size with disks managed by Portworx</strong></h2>
<pre><code class="language-plaintext">pxctl service pool expand --operation auto --size 1000 --uid &lt;pool-UUID&gt;</code></pre>
<ul>
  <li>By default, each pool can have a maximum of 6 drives. If required, you can use the runtime option <code>limit_drives_per_pool</code> to change this value.</li>
  <li>maximum of 12 disks for vShpere</li>
</ul>
<pre><code class="language-plaintext">pxctl service pool show</code></pre>
<pre><code class="language-plaintext">pxctl cluster provision-status</code></pre>
<h2><strong>Expand your storage pool size with disks not managed by Portworx</strong></h2>
<ul>
  <li><strong>ðŸ’€Omg, after adding the drives manually to each storage node do that:</strong></li>
</ul>
<pre><code class="language-plaintext">pxctl sv drive add --drive /dev/sde --operation start


pxctl service drive add --drive /dev/sde --operation status</code></pre>
<p>ðŸ˜´ðŸ˜´ When the status shows as <code>Drive /dev/sde already in use by PX.</code>, storage will be brought online automatically when this operation completes.</p>
<ul>
  <li><strong>Expand a pool size to consume the added drive capacity:</strong></li>
</ul>
<pre><code class="language-plaintext">pxctl service pool show</code></pre>
<pre><code class="language-plaintext">pxctl service pool update --resize &lt;pool-id&gt;</code></pre>
<h2><strong>Configure Kubernetes Cluster Autoscaler to Autoscale Storage Nodes</strong></h2>
<p><strong>ðŸŽ‰ðŸŽ‰ðŸŽ‰ðŸŽ‰ðŸŽ‰ </strong>preconfigure the node template and add the <code>portworx.io/provision-storage-node="true"</code> label.</p>
<blockquote>
  <p>Auto-scaling down is not supported. When using <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/README.md">Kubernetes Cluster Autoscaler</a>, make sure that auto-scaling down is disabled. Otherwise, drivesets might get orphaned and Portworx might lose quorum.</p>
</blockquote>
<ul>
  <li>OpenShift cluster <mark class="marker-yellow">does not require creating separate node pools for autoscaling Portworx storage nodes, </mark>but you must enable the <code>balanceSimilarNodeGroups</code> setting in the Cluster Autoscaler configuration to distribute new nodes evenly across zones so that each zone contains approximately the same number of nodes.</li>
  <li>To bring up autoscaled nodes as storageless, omit the label from the node pool's template.</li>
</ul>
<blockquote>
  <p>In vSphere Cloud Drive setups, when a node is down (<mark class="marker-yellow">but not marked unhealthy</mark>), any new node added by the Cluster Autoscaler may remain storage-less due to Portworx safety checks. Since<mark class="marker-yellow"> vSphere drives stay attached to the down node, the new node cannot attach those drives and will not convert to a storage node.</mark></p>
</blockquote>
<p>&nbsp;</p>
<p>&nbsp;</p>
