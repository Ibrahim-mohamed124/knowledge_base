<!--
title: Asynchronous Disaster Recovery
description: 
published: true
date: 2025-12-15T15:01:19.938Z
tags: 
editor: ckeditor
dateCreated: 2025-12-15T12:41:07.628Z
-->

<blockquote>
  <p>Cluster-wide operators are not migrated during a DR migration unless they are installed in the same namespace as the applications you want to migrate</p>
</blockquote>
<blockquote>
  <p>Asynchronous DR is supported for Kubevirt VMs with<mark class="marker-yellow"> PX RWX Block Volumes</mark></p>
</blockquote>
<figure class="image"><img src="/async-dr-detailed-c92b337de0ad41724209827cc94310ad.png"></figure>
<blockquote>
  <p>Incremental changes from Kubernetes or OpenShift applications and Portworx data are continuously sent to the destination cluster</p>
</blockquote>
<blockquote>
  <p>All worker nodes in the source and destination clusters must be able to access the Kubernetes API endpoints (port 6443)</p>
  <p>For an OpenShift based distribution, the source and destination clusters must be able to access ports in the <mark class="marker-yellow">17001–17020 range to enable communication between Portworx worker nodes.</mark></p>
  <p>A <mark class="marker-yellow">maximum of one StorageClass object is configured as the default</mark>. Having multiple default StorageClasses causes PVC migrations to fail.</p>
  <p><mark class="marker-yellow">Make sure the custom images are available from a registry that is accessible to both the source and destination clusters.</mark></p>
</blockquote>
<h2>Prepare</h2>
<ul>
  <li><strong>Install storkctl</strong></li>
</ul>
<pre><code class="language-plaintext">STORK_POD=$(oc get pods -n portworx -l name=stork -o jsonpath='{.items[0].metadata.name}') &amp;&amp;
oc cp -n portworx $STORK_POD:/storkctl/linux/storkctl ./storkctl
sudo mv storkctl /usr/local/bin &amp;&amp;
sudo chmod +x /usr/local/bin/storkctl</code></pre>
<ul>
  <li><strong>Create &nbsp;Service accounts&nbsp;</strong></li>
</ul>
<pre><code class="language-plaintext"> apiVersion: v1
 kind: ServiceAccount
 metadata:
   name: migration
   namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: migration-clusterrolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: migration
  namespace: default
---
apiVersion: v1
kind: Secret
metadata:
  name: migration
  namespace: default
  annotations: 
    kubernetes.io/service-account.name: migration
type: kubernetes.io/service-account-token</code></pre>
<p>&nbsp;</p>
<pre><code class="language-plaintext">SERVICE_ACCOUNT=migration
NAMESPACE=&lt;namespace&gt;
SERVER=https://&lt;SERVER-ADDRESS:PORT&gt;

SERVICE_ACCOUNT_TOKEN_COUNT=$(kubectl -n ${NAMESPACE} get secret -o=jsonpath='{.items[?(@.metadata.annotations.kubernetes\.io/service-account\.name=="'${SERVICE_ACCOUNT}'")].metadata.name}' | wc -w)
if [ ${SERVICE_ACCOUNT_TOKEN_COUNT} -gt 1 ]
then
  SERVICE_ACCOUNT_TOKEN_NAME=$(kubectl -n ${NAMESPACE} get secret -o=jsonpath='{.items[?(@.metadata.annotations.kubernetes\.io/service-account\.name=="'${SERVICE_ACCOUNT}'")].metadata.name}' | awk '{for(i=1;i&lt;=NF;i++){ if($i ~ /-token/){print $i} } }' | head -n 1)
else
  SERVICE_ACCOUNT_TOKEN_NAME=$(kubectl -n ${NAMESPACE} get secret -o=jsonpath='{.items[?(@.metadata.annotations.kubernetes\.io/service-account\.name=="'${SERVICE_ACCOUNT}'")].metadata.name}')
fi
SERVICE_ACCOUNT_TOKEN=$(kubectl -n ${NAMESPACE} get secret ${SERVICE_ACCOUNT_TOKEN_NAME} -o "jsonpath={.data.token}" | base64 --decode)
SERVICE_ACCOUNT_CERTIFICATE=$(kubectl -n ${NAMESPACE} get secret ${SERVICE_ACCOUNT_TOKEN_NAME} -o "jsonpath={.data['ca\.crt']}")

cat &lt;&lt;END
apiVersion: v1
kind: Config
clusters:
- name: default-cluster
  cluster:
    certificate-authority-data: ${SERVICE_ACCOUNT_CERTIFICATE}
    server: ${SERVER}
contexts:
- name: default-context
  context:
    cluster: default-cluster
    namespace: ${NAMESPACE}
    user: ${SERVICE_ACCOUNT}
current-context: default-context
users:
- name: ${SERVICE_ACCOUNT}
  user:
    token: ${SERVICE_ACCOUNT_TOKEN}
END</code></pre>
<pre><code class="language-plaintext">chmod +x create-migration-config.sh &amp;&amp; ./create-migration-config.sh &gt; ~/.kube/migration-config.conf</code></pre>
<h3><strong>Unidirectional ClusterPair</strong></h3>
<ul>
  <li>​A unidirectional ClusterPair establishes authentication from the source cluster to the destination cluster so resources and data can be migrated in one direction.</li>
</ul>
<pre><code class="language-plaintext">storkctl create clusterpair migration-cluster-pair \
  --namespace &lt;migrationnamespace&gt; \ # To create an admin ClusterPair, specify the admin namespace
  --dest-kube-file &lt;destination-kubeconfig-file&gt; \
  --src-kube-file &lt;source-kubeconfig-file&gt; \
  --provider s3 \
  --s3-endpoint s3.amazonaws.com \
  --s3-access-key &lt;s3-access-key&gt; \
  --s3-secret-key &lt;s3-secret-key&gt; \
  --s3-region &lt;s3-region&gt; \
  --unidirectional \ 
  --bucket &lt;name&gt;
  # Use --dest-ep and --src-ep when configuring asynchronous disaster recovery in cloud platforms.
  # --dest-ep &lt;host.fqdn:port&gt; \
  # --src-ep &lt;host.fqdn:port&gt; \
  # Starting from Stork version 25.4.1, You can skip the credentials vadiation using the --skip-validation flag 
  # --skip-validation</code></pre>
