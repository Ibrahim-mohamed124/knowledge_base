<!--
title: installer binary
description: 
published: true
date: 2025-10-25T10:39:37.725Z
tags: ocp, openshift, installation, implementation
editor: ckeditor
dateCreated: 2025-09-27T09:16:09.274Z
-->

<h1 style="text-align:center;"><strong>Openshfit-install</strong></h1>
<p style="text-align:center;">&nbsp;</p>
<p>Starting with OpenShift Container Platform 4.6, a full-stack installation on bare metal uses the OpenShift installer binary <code><strong>openshift-baremetal-install &nbsp;</strong></code>&nbsp;</p>
<p>&nbsp;</p>
<figure class="table">
  <table>
    <tbody>
      <tr>
        <td>Option</td>
        <td>Function</td>
      </tr>
      <tr>
        <td>explain</td>
        <td>explains the fields of InstallConfig APIs</td>
      </tr>
      <tr>
        <td>explain installconfig</td>
        <td>explains the fields of installconfig resource</td>
      </tr>
      <tr>
        <td>log-level debug</td>
        <td>enable debugging mode</td>
      </tr>
      <tr>
        <td>dir</td>
        <td>path to save the generated assets</td>
      </tr>
      <tr>
        <td>create install-config</td>
        <td>creates install-config.yaml</td>
      </tr>
      <tr>
        <td>create manifests</td>
        <td>creates k8s manifests out of install-config.yaml</td>
      </tr>
      <tr>
        <td>create ignition-conifgs</td>
        <td>creates ignition files out of k8s manifests</td>
      </tr>
      <tr>
        <td>create cluster</td>
        <td>deploy a cluster&nbsp;</td>
      </tr>
      <tr>
        <td>wait-for bootstrap-complete</td>
        <td>waits until the bootstrap installation phase ends</td>
      </tr>
      <tr>
        <td>wait-for install-complete</td>
        <td>waits until the cluster instillation ends</td>
      </tr>
      <tr>
        <td>destroy cluster</td>
        <td>delete the cluster</td>
      </tr>
      <tr>
        <td>gather</td>
        <td>get logs after kubernetes api is up and running</td>
      </tr>
    </tbody>
  </table>
</figure>
<p># &nbsp;before the installation: <code><strong>ssh-keygen -t rsa -b 4096 -N '' -f ${HOME}/.ssh/ocp4-cluster</strong></code></p>
<p># When using the pre-existing infrastructure installation method, you must specify the IP addresses of the bootstrap and control plane nodes when running the <code>openshift-install gather bootstrap</code> command.</p>
<p>The workflow of cluster creation:</p>
<ol>
  <li>complete the general requirements: provide the ocp4-cluster.pub key</li>
  <li>create a directory for the instillation: The OpenShift installer will create the installation directory if it does not exist.</li>
  <li>generate the install-config.yaml</li>
  <li>generate the k8s manifests: which contains the <code>MachineConfigs</code> object that is used in ignition files</li>
  <li>generate the ignition files :</li>
</ol>
<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Ignition files are valid for 24 hours, after which the included certificates expire. If the cluster installation fails, check the ignition &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;files to see if they are more than 24 hours old. If so, create new ignition files.</p>
<p>&nbsp; 6. deploy the cluster</p>
<p>! &nbsp; In case of UPI instead for using openshift-install create cluster, use the openshfit-install wait-for commands to moniter the cluster installation.</p>
<p>! In UPI, after the installation of bootstrap, it automatically triggers the installation of ocp on the already booted CoreOS hosts</p>
<p>! You can start using the <code>oc</code> command once the bootstrap has the Kubernetes API running on the temporary control plane.</p>
<ul>
  <li><code><strong>export KUBECONFIG=[HOME]/[instillation dir]/auth/kubeconfig</strong></code></li>
  <li><code><strong>Or use the password in kubeadmin-password file</strong></code></li>
</ul>
<p>Useful commands:</p>
<p>&nbsp; - &nbsp;<code><strong>watch 'oc get clusterversion; oc get clusteroperators; \</strong></code> &gt; <code><strong>oc get pods --all-namespaces | grep -v -E "Running|Completed"; oc get nodes'</strong></code></p>
<p>&nbsp; - <code><strong>oc get events -A -w</strong></code></p>
<h1 style="text-align:center;">Troubleshooting&nbsp;</h1>
<p style="text-align:center;">Deployment stages &gt; determine the stage by looking at the installation logs</p>
<figure class="table" style="text-align:center;">
  <table>
    <tbody>
      <tr>
        <td><code>Stage 1</code></td>
        <td><code>Stage 2</code></td>
        <td><code>Stage 3</code></td>
      </tr>
      <tr>
        <td>Bootstrap<br>BootKube&nbsp;</td>
        <td>Bootstrap<br>Temp control plane</td>
        <td>Production control plane&nbsp;</td>
      </tr>
      <tr>
        <td>
          <p>After bootstrap deployment by openshfit-installer, two systemd services in the node do some action:</p>
          <ul>
            <li style="text-align:justify;">release-images service: download the required images to start temp control plane</li>
            <li style="text-align:justify;">bootkube service: starts the temp control plane</li>
          </ul>
        </td>
        <td>After the control plane nodes boots, the bootstrap creates a ETCD cluster, and schedules the production control plane to the control nodes</td>
        <td>After complete deployment of production control plane, the clusterversionopeator finished the cluster deployment</td>
      </tr>
    </tbody>
  </table>
</figure>
<p>&nbsp;</p>
<p>Stage 1:</p>
<ul>
  <li>ssh to the bootstrap node using ssh key generated before the installation</li>
  <li>inspect journal logs of release-images and bootkube services</li>
  <li>inspect container logs using crictl cli</li>
</ul>
<p>Stage 2:</p>
<ul>
  <li>use oc or openshift-install to get the logs from the bootstrap <code><strong>export KUBECONFIG=${HOME}/ocp4-cluster/auth/kubeconfig</strong></code></li>
  <li><code><strong>openshift-install gather bootstrap --dir /path</strong></code> | this command gathers the logs and fetch them as a tarball<ul>
      <li>the archive contains:<ul>
          <li>bootstrap/journals/ &nbsp;bootstrap/containers/ &nbsp;control-plane/ &nbsp;failed-units.txt &nbsp;rendered-assets/ resources/ &nbsp;unit-status/</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><code><strong>oc adm node-logs -u kubelet master01, oc debug node/master01</strong></code></li>
  <li>Typical errors at this stage include:<ul>
      <li>Control plane nodes installation issues</li>
      <li>DNS resolution issues</li>
    </ul>
  </li>
</ul>
<p>Stage 3:</p>
<ul>
  <li>Typical error is OpenShift operators installation issues</li>
  <li>Instillation verification:</li>
</ul>
<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1. Verify that all the cluster nodes have their system clock synchronized with a Network Time Protocol (NTP) server. &gt; <code><strong>cat /etc/chrony.conf</strong></code></p>
<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2. Verify that all the cluster nodes are in a <code>Ready</code> status</p>
<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3. Check that all the cluster nodes are reporting usage metrics. &gt; <code><strong>oc adm top node</strong></code></p>
<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4. Ensure that there are no certificate signing requests (CSRs) pending approval. &gt; &nbsp;<code><strong>oc get csr | grep Pending</strong></code></p>
<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5. Confirm that the cluster version operator report shows that the OpenShift cluster is available and ready. &gt; &nbsp;<code><strong>oc get clusterversion</strong></code></p>
<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 6. Check that all the cluster operators are available and ready. &gt; <code><strong>oc get clusteroperators</strong></code></p>
<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 7. Verify that there are not any pods with scheduling or execution issues in the cluster. &gt; &nbsp;<code><strong>oc get pods --all-namespaces | grep -v -E 'Running|Completed'</strong></code></p>
<p>&nbsp;</p>
<ul>
  <li>ETCD verification:<ul>
      <li><code><strong>oc get pods -n openshift-etcd | grep etcd-master</strong></code></li>
      <li><code><strong>oc rsh -n openshift-etcd etcd-master01 &nbsp;&gt;&gt; etcdctl endpoint health --cluster</strong></code></li>
      <li><code><strong>etcdctl check perf --load="s,m,l"</strong></code> | small, medium, large</li>
      <li>For more detailed etcd storage performance information<ul>
          <li><code><strong>podman run --volume /var/lib/etcd:/var/lib/etcd:Z quay.io/openshift-scale/etcd-perf</strong></code> | from any control plane</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>API Health verification:<ul>
      <li><code><strong>dig api.ocp4.example.com</strong></code> &nbsp;| resolve the ip of the api loadbalancer</li>
      <li><code><strong>curl -k https://api.ocp4.example.com:6443/version</strong></code> &nbsp;| check the version of api</li>
      <li><code><strong>curl -kIs&nbsp;</strong></code> <code><strong>https://console-openshift-console.apps.ocp4.example.com</strong></code> | check the connection of the console</li>
    </ul>
  </li>
  <li>Openshift registry health:<ul>
      <li><code><strong>oc -n openshift-image-registry get deployment.apps/image-registry</strong></code></li>
      <li><code><strong>oc -n openshift-image-registry &nbsp;get pods -o wide</strong></code></li>
      <li><code><strong>oc debug node/master01 ; &nbsp;curl -Ik https://image-registry.openshift-image-registry.svc:5000/healthz</strong></code></li>
      <li>Verify that the internal registry deployment is using persistent storage. Also, ensure that the image registry operator is in the <code>Managed</code> management state.<ul>
          <li><code><strong>oc get configs.imageregistry.operator.openshift.io cluster -o yaml</strong></code></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Openshift Ingress health:<ul>
      <li>Verify that the wildcard DNS record for applications, <code>*.apps.ocp4.example.com</code>, is configured to use the external load balancer IP address<ul>
          <li><code><strong>dig test.apps.ocp4.example.com</strong></code></li>
        </ul>
      </li>
      <li>Check that you can access an application exposed by an OpenShift Ingress route</li>
    </ul>
  </li>
  <li>Openshift cluster Network<ul>
      <li><code><strong>oc get network.config/cluster -o yaml</strong></code> | to get pods and svc cidrs</li>
    </ul>
  </li>
</ul>
<h4>OpenShift Machine API</h4>
<p>The OpenShift Machine API is the component that defines and manages the OpenShift <code>Machines</code> resource. The OpenShift <code>Machines</code> resource represents the OpenShift cluster nodes. The OpenShift Machine API:</p>
<ul>
  <li>Creates, updates, and deletes <code>Machines</code></li>
  <li>Creates the infrastructure (instance or VM) for the node</li>
</ul>
<p>You can use the OpenShift <code>MachineSets</code> resource to control sets of <code>Machines</code>. A <code>Machineset</code> represents:</p>
<ul>
  <li>A set of <code>Machines</code></li>
  <li>An abstraction of the underlying infrastructure</li>
</ul>
<p>&nbsp;</p>
<ul>
  <li><code><strong>oc get machines -n openshift-machine-api</strong></code></li>
  <li><code><strong>oc get machinesets -n openshift-machine-api</strong></code></li>
  <li><code><strong>sosreport -k crio.all=on -k crio.logs=on</strong></code> &nbsp;| on the nodes</li>
</ul>
<p>@ to access the cluster metrics from redhat openshfit cluster manager console get the cluster id&nbsp;</p>
<ul>
  <li><code><strong>oc get clusterversion \</strong></code>
    &gt; <code><strong>-o jsonpath='{.items[].spec.clusterID}{"\n"}'</strong></code></li>
</ul>
<p>&nbsp;</p>
