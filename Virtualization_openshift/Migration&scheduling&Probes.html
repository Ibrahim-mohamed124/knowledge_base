<!--
title: Migration & Scheduling & Probes
description: 
published: true
date: 2025-11-23T11:11:38.788Z
tags: 
editor: ckeditor
dateCreated: 2025-11-23T09:00:45.253Z
-->

<h1>Migrating Virtual Machines from Foreign Hypervisors</h1>
<ul>
  <li>With the migration toolkit for virtualization (MTV), multiple VMs cloud be migrated from a compatible hypervisor, such as VMware vSphere or Red&nbsp;Hat Virtualization, to OpenShift Virtualization.</li>
  <li>MTV operator provide ForkliftController object that has these APIs:</li>
</ul>
<figure class="image image_resized" style="width:100%;"><img src="/mtv_resources-v2.svg"></figure>
<ol>
  <li><code><strong>Provider:</strong></code>Contains the API endpoint and the credentials to<mark class="marker-yellow"> interact with the source and destination virtualization systems.</mark></li>
  <li><code><strong>NetworkMap:</strong></code>Declares the source network on the remote virtualization system and the destination network for the imported VMs in OpenShift Virtualization.</li>
  <li><code><strong>StorageMap:</strong></code>Declares the source datastores on the remote virtualization system and the destination storage class for the imported VMs in OpenShift Virtualization.</li>
  <li><code><strong>Plan:</strong></code><mark class="marker-yellow">Contains the parameters to migrate the VMs from the source </mark><code>Provider</code> to OpenShift Virtualization.</li>
  <li><code><strong>Migration:</strong></code><mark class="marker-yellow">Executes the VM migration as defined in the plan CRD</mark>. The migration CRD also tracks the status of each migration phase:<ul>
      <li><code>Initialize</code>: Initialize migration.</li>
      <li><code>DiskAllocation</code>: Allocate disks.</li>
      <li><code>ImageConversion</code>: Convert image to <code>kubevirt</code> format.</li>
      <li><code>DiskTransferV2v</code>: Copy disks.</li>
      <li><code>VirtualMachineCreation</code>: Create the VM.</li>
    </ul>
  </li>
</ol>
<h4>VM Providers</h4>
<ul>
  <li>The <code>Provider</code> CR stores attributes that enable MTV to connect to and interact with the source and target VM providers. The MTV can interact with the following hypervisors:<ul>
      <li>VMware vSphere</li>
      <li>Red&nbsp;Hat Virtualization (RHV)</li>
      <li>OpenStack</li>
      <li>Remote OpenShift Virtualization clusters</li>
    </ul>
  </li>
</ul>
<blockquote>
  <p>The MTV automatically creates the <code>host</code> provider for the local OpenShift Virtualization cluster in the <code>openshift-mtv</code> namespace.</p>
</blockquote>
<ul>
  <li><strong>Migration</strong> → <strong>Providers for virtualization</strong> and click <strong>Create Provider</strong>.</li>
</ul>
<h4>Migration Plan</h4>
<ul>
  <li>The <code>Plan</code> CR defines a migration plan with a list of VMs from the same VM provider and associated infrastructure mappings.</li>
  <li>The <code>Migration</code> CR runs a migration plan.</li>
  <li><strong>Migration</strong> → <strong>Plans for virtualization</strong> and click <strong>Create plan</strong></li>
</ul>
<h6><strong>Cold migration</strong></h6>
<ul>
  <li>The source VM is shut down, and then the data is transferred to the target OpenShift cluster. After the transfer is complete, the VM is powered on, in OpenShift Virtualization.</li>
</ul>
<h6><strong>Warm migration</strong></h6>
<ul>
  <li>The <i>precopy stage</i> uses changed block tracking (CBT) snapshots on the source VM provider to<mark class="marker-yellow"> copy the VM data while the VM is running</mark>. You must enable CBT for each source VM and each VM disk to use warm migration.</li>
  <li>The <i>cutover stage</i><mark class="marker-yellow"> shuts down the VM and copies the remaining data to the target cluster</mark>. The VM is then started on OpenShift Virtualization. You can manually start the cutover stage from the web console, or you can schedule a cutover time in the migration plan manifest.</li>
</ul>
<h4>MTV worker pods</h4>
<ul>
  <li>The importer pod transfers the VM image from the source <code>Provider</code> to a data volume in the target OpenShift cluster.</li>
  <li>The conversion pod runs the <code>virt-v2v</code> tool to install and configure the required device drivers on the VM root disk for the VM to run on OpenShift Virtualization, and then transfers the disk to a PVC.</li>
</ul>
<blockquote>
  <p>To clean up the temporary resources that the MTV creates during the migration, such as the migration pods and temporary PVCs, you must archive the migration plan.&nbsp;</p>
</blockquote>
<h4>Importing VMs from VMware</h4>
<ul>
  <li>The MTV operator uses the VDDK SDK to accelerate transferring virtual disks from VMware vSphere. So, creating VDDK container image is essential.</li>
</ul>
<pre><code class="language-plaintext">mkdir -vp /tmp/vddk
cd /tmp/vddk
tar -xzf VMware-vix-disklib-version.x86_64.tar.gz

FROM registry.access.redhat.com/ubi8/ubi-minimal
USER 1001
COPY vmware-vix-disklib-distrib /vmware-vix-disklib-distrib
RUN  mkdir -vp /opt
ENTRYPOINT ["cp", "-vr", "/vmware-vix-disklib-distrib", "/opt"]

TAG=&lt;registry_route_or_server_path&gt;/vddk:latest
podman build . -t ${TAG}
podman push ${TAG}</code></pre>
<h1>Controlling the Scheduling of Virtual Machines</h1>
<h4>Configure the Run Strategy</h4>
<ul>
  <li>A VM's run strategy <mark class="marker-yellow">determines the behavior of a VMI according to a series of conditions</mark>, which are defined in the <code>.spec.running</code> or the <code>.spec.runStrategy</code> parameter.<ul>
      <li><code><strong>.spec.running</strong></code>
        <ul>
          <li>The <code>.spec.running</code> Boolean value specifies whether the VM is running.<ul>
              <li><code><strong>true</strong></code>
                <ul>
                  <li>When the Boolean is set to <code>true</code>, Red&nbsp;Hat OpenShift Virtualization ensures that the VM is always running. OpenShift Virtualization restarts the VM if it fails. OpenShift Virtualization also restarts the VM when you stop it by gracefully shutting down the operating system or when you delete the VMI resource.</li>
                </ul>
              </li>
              <li><code><strong>false</strong></code>
                <ul>
                  <li>When you set the <code>.spec.running</code> Boolean to <code>false</code>, OpenShift Virtualization ensures that the VM is not running.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>OpenShift Virtualization automatically updates the Boolean when you control the VM by using the <code>virtctl</code> command or by using the web console. If you run the <code>virtctl stop</code> command or click <strong>Actions</strong> → <strong>Stop</strong> in the OpenShift web console, then OpenShift Virtualization switches the <code>.spec.running</code> Boolean to <code>false</code>. If you run the <code>virtctl start</code> command or click <strong>Actions</strong> → <strong>Start</strong> in the OpenShift web console, then OpenShift Virtualization switches the <code>.spec.running</code> Boolean to <code>true</code>.<ul>
      <li><code><strong>.spec.runStrategy</strong></code>
        <ul>
          <li>The <code>.spec.runStrategy</code> parameter provides more control over the VM status than the <code>.spec.running</code> Boolean. For example, when the <code>.spec.running</code> Boolean is <code>true</code>, you cannot stop a VM by gracefully shutting down its operating system, because OpenShift Virtualization detects that the VM is not running and then restarts it.</li>
          <li>The <code>.spec.runStrategy</code> parameter accepts any of the following values:<ul>
              <li><code><strong>RerunOnFailure</strong></code>
                <ul>
                  <li>Restarts the VM when it fails. If you gracefully shut down the operating system from inside the VM, then OpenShift Virtualization does not restart the VM.</li>
                </ul>
              </li>
              <li><code><strong>Always</strong></code>
                <ul>
                  <li>Ensures that the VM is always running. Using this value has the same effect as setting the <code>.spec.running</code> Boolean to <code>true</code>.</li>
                </ul>
              </li>
              <li><code><strong>Halted</strong></code>
                <ul>
                  <li>Ensures that the VM is not running. Using this value has the same effect as setting the <code>.spec.running</code> Boolean to <code>false</code>.</li>
                </ul>
              </li>
              <li><code><strong>Manual</strong></code>
                <ul>
                  <li>Performs no automatic action. If the VM fails, then OpenShift Virtualization does not restart it. You must manage the VM by using the <code>virtctl start</code> or <code>virtctl stop</code> command, or by using the OpenShift web console.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>The <code>.spec.running</code> and <code>.spec.runStrategy</code> parameters are mutually exclusive. If you specify both parameters, then OpenShift Virtualization returns an error.</li>
</ul>
<h1>Configuring Health Probes for Virtual Machines</h1>
<ul>
  <li>In addition to the normal pod probs VMs has <strong>Guest agent ping probe:</strong>
    <ul>
      <li>The guest agent ping probe uses the <code>guest-ping</code> command to determine whether the QEMU guest agent is running on the virtual machine.&nbsp;</li>
    </ul>
  </li>
</ul>
<h4>Configure a Watchdog Device</h4>
<ul>
  <li>The watchdog feature detects and restarts unresponsive operating systems.<ul>
      <li>A hardware component, which sets by default a 30-second timer. You can adjust this timer if needed. When the timer expires, <mark class="marker-yellow">the component triggers a system restart</mark>. On bare-metal machines, a chipset provides the feature. For VMs, <mark class="marker-yellow">OpenShift Virtualization emulates the chipset.</mark></li>
      <li>A software component, which the <mark class="marker-yellow">operating system runs, to reset the hardware timer regularly to prevent it from expiring.</mark> If the operating system hangs, then the software component also hangs and cannot refresh the timer. Then, the timer expires and the system restarts.</li>
    </ul>
  </li>
</ul>
<pre><code class="language-plaintext">apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
...output omitted...
spec:
  ...output omitted...
  template:
    metadata:
    ...output omitted...
    spec:
      domain:
        cpu:
          ...output omitted...
        devices:
          disks:
          - bootOrder: 1
            disk:
              bus: virtio
            name: mariadb-server
          interfaces:
          - macAddress: "02:48:09:00:00:00"
            masquerade: {}
            name: default
          networkInterfaceMultiqueue: true
          rng: {}
          watchdog:
            i6300esb: 1
              action: poweroff 2
            name: mywatchdog</code></pre>
<figure class="table">
  <table style="background-color:rgb(255, 255, 255);">
    <tbody>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg" alt="1"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">OpenShift Virtualization can emulate only the Intel 6300ESB chipset.</td>
      </tr>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg" alt="2"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">Action to perform when the watchdog triggers. The <code>poweroff</code> action stops the VM, and then OpenShift Virtualization restarts the VM according to the <code>.spec.running</code> or the <code>.spec.runStrategy</code> parameter. Other options for this parameter include the <code>reset</code> or <code>shutdown</code> actions. The references section provides more details about the watchdog actions</td>
      </tr>
    </tbody>
  </table>
</figure>
<h1>Live Migrating a Virtual Machine</h1>
<ul>
  <li><i>Live migration</i> is the process of moving a running VMI to a different cluster node without disrupting access or the virtual workload, and is similar to the VMware vMotion feature for VMs.&nbsp;</li>
  <li>the VM must meet the following conditions:<ul>
      <li>The underlying PVC must use ReadWriteMany (RWX) access mode. If you do not configure the access mode and volume mode parameters in the <code>DataVolume</code> object for a VM, then the defined values in the storage profile for the storage class are applied to the PVC.</li>
      <li>The pod network must not be configured with the <code>bridge</code> binding type</li>
      <li>Ports <code>49152</code> and <code>49153</code> must be available in the VM's <code>virt-launcher</code> pod; live migration fails if these ports are specified in a <code>masquerade</code> network interface.</li>
    </ul>
  </li>
</ul>
<h4>Limitations of Live Migrations</h4>
<ul>
  <li>adjust the default settings for live migration limits and timeouts by editing the <code>HyperConverged</code> CR<ul>
      <li><code><strong>parallelMigrationsPerCluster: </strong></code>The maximum number of migrations that can run in parallel in the cluster. <strong>The default value is 5</strong>.</li>
      <li><code><strong>parallelOutboundMigrationsPerNode: </strong></code>The maximum number of parallel outbound migrations per node. <strong>The default value is 2</strong>.</li>
      <li><code><strong>bandwidthPerMigration: </strong></code>Limits the bandwidth, in MiB/s, that is used for each migration. <strong>The default value is 0, which is unlimited</strong>.</li>
      <li><code><strong>completionTimeoutPerGiB: </strong></code>If the migration exceeds the defined time, in seconds per GiB of memory, then the migration is canceled. The size of the migrating disks is included in the calculation if you use the <code>BlockMigration</code> migration method. <strong>The default value is 800</strong>.</li>
      <li><code><strong>progressTimeout: </strong></code>The migration is canceled if the memory copy fails to make progress in this time (in seconds). <strong>The default value is 150</strong>.</li>
    </ul>
  </li>
</ul>
<h4>Configuring a Dedicated Network for Live Migrations</h4>
<ul>
  <li>To create a dedicated network for live migration, you must first use the Kubernetes NMState operator to configure a bridge network on your cluster nodes. The node NICs that are used for the bridge network must be connected to the same VLAN. You must then create a network attachment definition for the dedicated network in the <code>openshift-cnv</code> project.</li>
  <li>After you create the network attachment definition for the bridge, edit the live migration configuration in the <code>HyperConverged</code> CR.&nbsp;</li>
</ul>
<pre><code class="language-plaintext">apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
...output omitted...
spec:
...output omitted...
  liveMigrationConfig:
    completionTimeoutPerGiB: 800
    network: live-migration-network 1
    parallelMigrationsPerCluster: 5
    parallelOutboundMigrationsPerNode: 2
    progressTimeout: 150
...output omitted...</code></pre>
<figure class="table">
  <table style="background-color:rgb(255, 255, 255);">
    <tbody>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg" alt="1"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">Specify the name of the network attachment definition object to use for live migrations.</td>
      </tr>
    </tbody>
  </table>
</figure>
<h3>Live Migration Strategies</h3>
<ul>
  <li>A live migration transfers, or converges, a source VM's state, which is mostly its memory, to a target VM on another node.&nbsp;<ul>
      <li><code><strong>pre-copy:</strong></code>The default strategy for live migrations. A target VM is created but the guest keeps running on the source VM. The VM state is then copied to the target VM until the entire VM state is transferred. <mark class="marker-yellow">When the VM state is transferred to the target VM, the guest starts running from the target VM and the source VM is deleted.</mark></li>
      <li><code><strong>post-copy:</strong></code><mark class="marker-yellow">A target VM is created and the guest starts running on the target VM.</mark> The source VM begins to send its state in chunks to the target VM. If the guest that is running on the target VM requests memory that is currently available only on the source VM, then that chunk of memory is copied to the target VM. <mark class="marker-yellow">The source VM is deleted after its state is copied to the target VM. This strategy consumes less bandwith, but is slower than the </mark><code><mark class="marker-yellow">pre-copy</mark></code><mark class="marker-yellow"> method</mark>. This strategy also prevents a single source of truth for the VM state because both the target and source VMs contain parts of the VM's state<mark class="marker-yellow">. If either VM crashes, then the state cannot be recovered.</mark></li>
      <li><code><strong>auto-converge:</strong></code>This strategy<mark class="marker-yellow"> enhances the </mark><code><mark class="marker-yellow">pre-copy</mark></code><mark class="marker-yellow"> strategy to ensure a successful migration. </mark>Similar to the <code>pre-copy</code> strategy, <mark class="marker-yellow">the guest keeps running on the source VM while the VM state is copied to the target VM. </mark>However, if the migration cannot complete before the source VM mutates or modifies the VM state, then <mark class="marker-yellow">the guest CPU is throttled on the source VM. Guest CPU throttling is gradually increased until the migration is successful.</mark> If the migration can complete fast enough, then the guest CPU is not throttled or is negligibly throttled.</li>
    </ul>
  </li>
</ul>
<blockquote>
  <p>The <code>pre-copy</code> strategy is appropriate for most situations. However, the <code>pre-copy</code> strategy brings a risk that by the time a VM state chunk is copied from the source VM to the target VM, the guest on the source VM already mutated or modified the chunk.&nbsp;</p>
</blockquote>
<ul>
  <li>To change the migration strategy at the cluster level, you must edit the <code>HyperConverged</code> CR in the <code>openshift-cnv</code> project</li>
</ul>
<pre><code class="language-plaintext">apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
  namespace: openshift-cnv
...output omitted...
spec:
...output omitted...
  liveMigrationConfig:
    allowAutoConverge: true
    allowPostCopy: false
...output omitted...</code></pre>
<h4>Live Migration Policy</h4>
<ul>
  <li>A live migration policy declares the migration configuration to apply to a VM or to a group of VMs.&nbsp;</li>
  <li>Live migration policies use VM and project labels as selectors for the policy.</li>
</ul>
<pre><code class="language-plaintext">apiVersion: migrations.kubevirt.io/v1alpha1
kind: MigrationPolicy
metadata:
  name: prod-database-policy 1
spec:
  allowAutoConverge: true
  selectors:
    namespaceSelector: 2
      database-workloads: "True"
    virtualMachineInstanceSelector: 3
      kubevirt.io/environment: "production"</code></pre>
<figure class="table">
  <table style="background-color:rgb(255, 255, 255);">
    <tbody>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg" alt="1"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">The name of the migration policy</td>
      </tr>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg" alt="2"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">The labels for the projects that are impacted by the migration policy</td>
      </tr>
      <tr>
        <td style="padding:0px;vertical-align:top;">
          <figure class="image image_resized" style="width:22px;"><img src="https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg" alt="3"></figure>
        </td>
        <td style="padding:0px;vertical-align:top;">The labels for the VMs that are impacted by the migration policy</td>
      </tr>
    </tbody>
  </table>
</figure>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
