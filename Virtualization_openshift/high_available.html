<!--
title: HA
description: 
published: true
date: 2025-11-26T18:41:49.269Z
tags: 
editor: ckeditor
dateCreated: 2025-11-25T15:29:40.716Z
-->

<h4>Fencing Hosts for VM Integrity</h4>
<ul>
  <li><i>fencing: is the isolation of &nbsp;misbehaved vm to prevent running the same workload in more than one host , which leads to data corruption, and eventually the service will be down.</i></li>
  <li><i>remediation: &nbsp; is the creation of new vm after a failure</i></li>
</ul>
<blockquote>
  <p>A traditional hypervisor uses an out-of-band management agent to fence a nonresponsive host. The <mark class="marker-yellow">agent forces a power-off</mark>, which ensures that the host and its virtual machines are down. <mark class="marker-yellow">Only then does the agent start the virtual machine on a new host.</mark></p>
</blockquote>
<ul>
  <li>in OpenShift: Fencing is a remediation method that reboots and deletes <code>Machine</code> custom resource definitions to solve problems with automatically provisioned nodes.</li>
  <li><strong>Machine health checks:&nbsp;</strong>
    <ul>
      <li>Machine health checks<mark class="marker-yellow"> automatically remediate an unhealthy machine (worker nodes)</mark>, which is the host for a node, if the machine exists in a particular machine pool.</li>
      <li>When the <code>MachineHealthCheck</code> controller detects that a node is in the <code>NotReady</code> state, it removes the associated <code>Machine</code> resource, and the node is deleted from the pool host.</li>
    </ul>
  </li>
  <li><strong>Sticky sessions:</strong>
    <ul>
      <li>Sticky sessions enable stateful application traffic by ensuring that all traffic hits the same endpoint.</li>
    </ul>
  </li>
</ul>
<pre><code class="language-plaintext">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    router.openshift.io/cookie_name: "hello"
...output omitted...</code></pre>
<h4>The Node Maintenance Operator</h4>
<ul>
  <li><code>NodeMaintenance</code> custom resource: declaratively place nodes into maintenance</li>
  <li>During node drain, OpenShift Virtualization migrates VMs &nbsp;with &nbsp;liveMigrate strategy live. For VMs with the eviction strategy of <code>None</code>, OpenShift Virtualization shuts down the VMs, and then restarts them on another node.</li>
</ul>
<blockquote>
  <p>Even though the eviction strategy is set, some VMs might not support live migration. For example, OpenShift Virtualization can live migrate only VMs with storage that supports the <code>ReadWriteMany</code> (RWX) access mode.</p>
  <p>These VMs prevent the drain process from completing. A cluster administrator must manually shut down the VM or disable the live migration.</p>
</blockquote>
<h4>Pod disruption budget</h4>
<ul>
  <li><mark class="marker-yellow">A pod disruption budget resource is created and deleted with each VMI </mark>to prevent the <code>virt-launcher</code> pod from being deleted before the live migration process is completed.</li>
  <li>With a pod disruption budget resource, the node drain is blocked until the replacement pods for the application are scheduled in another node and become ready.</li>
</ul>
<h2>Configuring Virtual Machines to Survive Node Failure</h2>
<h4>Scheduler Profiles</h4>
<ul>
  <li>The OpenShift scheduler profile controls how OpenShift schedules pods on nodes. The following scheduler profiles are available:<ul>
      <li><code><strong>LowNodeUtilization</strong></code>
        <ul>
          <li>Attempts to spread pods evenly across nodes for low resource usage per node.</li>
        </ul>
      </li>
      <li><code><strong>HighNodeUtilization</strong></code>
        <ul>
          <li>Attempts to place as many pods on as few nodes as possible. This approach minimizes the node count and creates high resource usage per node.</li>
        </ul>
      </li>
      <li><code><strong>NoScoring</strong></code>
        <ul>
          <li>A low latency profile that strives for the quickest scheduling cycle by disabling all scoring plug-ins.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<h3>Eviction Strategies</h3>
<ul>
  <li>Eviction strategies determine the appropriate actions to take with orphaned resources, such as pods, persistent volumes (PVs), and virtual machines (VMs) when the cluster nodes are not ready.</li>
  <li>Eviction strategies determine whether OpenShift moves the VMs on the failed node to another node or terminates them.</li>
  <li><code><strong>LiveMigrate</strong></code>
    <ul>
      <li>OpenShift migrates live, to ensure that the VM is not interrupted if you place the node into maintenance or if you drain it. This eviction <mark class="marker-yellow">strategy is the default. Non-migrateable VMs with this eviction strategy might prevent nodes from draining or might block a node upgrade because OpenShift does not evict the VM from the node, and you must shut down manually the VM</mark>.</li>
    </ul>
  </li>
  <li><code><strong>LiveMigrateIfPossible</strong></code>
    <ul>
      <li>If users do not request a live migration on VMs, then OpenShift terminates non-migrateable VMs during evictions.</li>
    </ul>
  </li>
  <li><code><strong>None</strong></code>
    <ul>
      <li>OpenShift does not migrate the VM, and it restarts or terminates the VM depending on the run strategy.</li>
    </ul>
  </li>
</ul>
<blockquote>
  <p>VMs with a live migration strategy must have a persistent volume claim (PVC) with a shared <code>ReadWriteMany</code> (RWX) access mode.</p>
</blockquote>
<h3>Node Failures</h3>
<ul>
  <li>VMs are scheduled only on nodes from which the control plane received a <code>virt-handler</code> heartbeat.</li>
  <li>It might take up to five minutes for the <code>virt-handler</code> daemon set and Kubernetes to detect the failure.</li>
</ul>
<blockquote>
  <p><mark class="marker-yellow">If the </mark><code><mark class="marker-yellow">virt-handler</mark></code><mark class="marker-yellow"> daemon set loses the connection to the cluster's API server, then the node cannot communicate its status. The node enters a failed state, and the remaining VMs cannot migrate to the healthy nodes.</mark></p>
</blockquote>
<ul>
  <li>a watchdog device to verify the state of the VM's guest OS, and act according to the run strategy. The VM's watchdog service monitors only for guest OS failures, and it does not detect application failures.</li>
  <li><code><strong>poweroff</strong></code>
    <ul>
      <li>The VM powers off immediately. If the <code>spec.running</code> Boolean is set to <code>true</code>, or if the <code>spec.runStrategy</code> parameter is not set to <code>manual</code>, then the VM reboots.</li>
    </ul>
  </li>
  <li><code><strong>reset</strong></code>
    <ul>
      <li>The VM reboots in place, and the guest OS cannot react.</li>
    </ul>
  </li>
  <li><code><strong>shutdown</strong></code>
    <ul>
      <li>The VM gracefully powers off by stopping all services.</li>
    </ul>
  </li>
</ul>
<h4>Standard Remediation for Failed Nodes</h4>
<ul>
  <li><mark class="marker-yellow">If a host fails the health check,</mark> then the <mark class="marker-yellow">bare metal machine controller drains the remaining VMs on the node and reschedules the workloads to healthy nodes that the machine set owns</mark>.&nbsp;</li>
  <li>After OpenShift drains all the VMs and<mark class="marker-yellow"> the node registers itself again to the cluster</mark>, <mark class="marker-yellow">the bare metal machine controller restores the annotations and labels from the unhealthy node to the new node</mark>. ????? really?</li>
  <li>To limit the disruptive impact of the host deletion<mark class="marker-yellow">, the controller drains and deletes one node at a time</mark>. If multiple unhealthy hosts exceed the <code><mark class="marker-yellow">maxUnhealthy</mark></code><mark class="marker-yellow"> specified threshold for the targeted pool of hosts,</mark> then<mark class="marker-yellow"> remediation stops and requires manual intervention.</mark></li>
  <li>A machine health check remediates a host immediately if the <code>Machine</code> resource enters the <code>Failed</code> status.</li>
</ul>
<h4>Power-based Remediation for Failed Bare Metal Nodes</h4>
<ul>
  <li>Instead of reprovisioning the nodes, power-based remediation uses <mark class="marker-yellow">a power controller to power off an inoperable node</mark>. This type of remediation is also called <i><mark class="marker-yellow">power fencing</mark></i>. OpenShift uses the <code>MachineHealthCheck</code> controller to detect faulty bare metal nodes.<mark class="marker-yellow"> Power-based remediation is fast and reboots faulty nodes instead of removing them from the cluster.</mark></li>
  <li>Enables the recovery of control plane nodes.</li>
  <li>Reduces the risk of data loss in hyperconverged environments.</li>
</ul>
<h3>Self Node Remediation Operator</h3>
<ul>
  <li>Available in<mark class="marker-yellow"> non-IPI bare metal clusters</mark>, the Self Node Remediation Operator<mark class="marker-yellow"> runs on the cluster nodes</mark> and <mark class="marker-yellow">reboots unhealthy nodes</mark>. This remediation strategy minimizes downtime for stateful applications and <code>ReadWriteOnce</code> (RWO) volumes, and restores compute capacity if <mark class="marker-yellow">transient failures occur.</mark></li>
  <li>The Self Node Remediation Operator creates a <code>SelfNodeRemediationConfig</code> CR in the operator's namespace.If the Self Node Remediation Operator detects a change in the <code>SelfNodeRemediationConfig</code> CR, then it re-creates the Self Node Remediation daemon set.</li>
  <li>The operator uses the <code><mark class="marker-yellow">MachineHealthCheck</mark></code><mark class="marker-yellow"> controller to detect the health of a node in the cluster.</mark> If an unhealthy node is detected according to the <code>SelfNodeRemediationConfig</code> CR, then the <code>MachineHealthCheck</code> controller creates the <code>SelfNodeRemediation</code> CR to trigger the Self Node Remediation Operator. Then, the operator uses the <code>SelfNodeRemediationTemplate</code> custom resource definition (CRD) to define the remediation strategy for the unhealthy nodes.</li>
</ul>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
