<!--
title: Anthos Upgrades
description: 
published: true
date: 2026-02-28T19:47:57.254Z
tags: 
editor: ckeditor
dateCreated: 2026-02-12T16:07:34.419Z
-->

<blockquote>
  <p>This documents assumes that all the clusters are updated to the recommended features and the cluster is not advanced cluster</p>
</blockquote>
<h1>Overview of cluster upgrades</h1>
<h6>Essentials&nbsp;</h6>
<ul>
  <li>The gkectl command line version must be the same as the target version of the clusters</li>
  <li>The Admin cluster version must be equal or higher than the User cluster version</li>
  <li>The gkectl command line version cannot be higher than the current cluster version by 2 minor versions</li>
</ul>
<h6>Upgrade sequence</h6>
<ol>
  <li>Upgrade the admin workstation</li>
  <li>Upgrade the admin cluster</li>
  <li>Upgrade the user clusters, one at a time</li>
</ol>
<blockquote>
  <p>For Controlplane V2, the user cluster control plane nodes are upgraded during the user cluster upgrade, and the admin cluster control plane nodes are upgraded during the admin cluster upgrade.</p>
</blockquote>
<blockquote>
  <p>When upgrading user clusters, you can choose to upgrade the user cluster as a whole (meaning you can upgrade the control plane and all node pools in the cluster), or you can upgrade the user cluster's control plane and leave the node pools at the current version. Which approach you take depends on several factors, such as:</p>
  <ul>
    <li>The environment (production or non-production) that the cluster is in.</li>
    <li>The length of your maintenance window.</li>
    <li>The version of the user cluster.</li>
  </ul>
</blockquote>
<h6>Upgrade tool</h6>
<ul>
  <li>The command-line tool <code>gkectl</code>, which you run on your admin workstation. Before the upgrade, you modify your user cluster <a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/user-cluster-configuration-file-latest#nodepool-gkeonpremversion-field">configuration file</a> to set the target version for the cluster's control plane and optionally, for the node pools. You specify this file on the command line to <code>gkectl</code>.</li>
</ul>
<h1>Best practices</h1>
<h6>Upgrade checklist</h6>
<ul class="todo-list">
  <li><a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/upgrade-best-practices#estimate_the_time_commitment_and_plan_a_maintenance_window"><span class="text-tiny"><label class="todo-list__label"><u><input type="checkbox" disabled="disabled"></u><span class="todo-list__label__description"><u>Estimate the time commitment and plan a maintenance window</u></span></label><span class="todo-list__label__description"> for the upgrade.</span></span></a></li>
  <li><a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/upgrade-best-practices#back_up_the_user_and_admin_cluster"><span class="text-tiny"><label class="todo-list__label"><input type="checkbox" disabled="disabled"><span class="todo-list__label__description">Prepare backups for the user clusters and admin cluster.</span></label></span></a></li>
  <li><a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/upgrade-best-practices#review_the_use_of_poddisruptionbudgets"><span class="text-tiny"><label class="todo-list__label"><input type="checkbox" disabled="disabled"><span class="todo-list__label__description">Check for </span></label><code><span class="todo-list__label__description">PodDisruptionBudgets</span></code><span class="todo-list__label__description"> in user clusters.</span></span></a></li>
  <li><a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/upgrade-best-practices#review_the_available_ip_addresses"><span class="text-tiny"><label class="todo-list__label"><input type="checkbox" disabled="disabled"><span class="todo-list__label__description">Review the available IP addresses and needs.</span></label></span></a></li>
  <li><a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/upgrade-best-practices#check_cluster_utilization"><span class="text-tiny"><label class="todo-list__label"><input type="checkbox" disabled="disabled"><span class="todo-list__label__description">Check the user cluster utilization and available resources.</span></label></span></a></li>
  <li><a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/upgrade-best-practices#check_vsphere_utilization"><span class="text-tiny"><label class="todo-list__label"><input type="checkbox" disabled="disabled"><span class="todo-list__label__description">Check the vSphere cluster capacity for available resources such as CPU, memory, and ready times.</span></label></span></a></li>
  <li><a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/upgrade-best-practices#check_cluster_health"><span class="text-tiny"><label class="todo-list__label"><input type="checkbox" disabled="disabled"><span class="todo-list__label__description">Check the cluster health and configuration to resolve problems before the upgrade.</span></label></span></a></li>
  <li><a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/upgrade-best-practices#decide_how_to_upgrade_each_user_cluster"><span class="text-tiny"><label class="todo-list__label"><input type="checkbox" disabled="disabled"><span class="todo-list__label__description">Decide how to upgrade each user cluster.</span></label></span></a></li>
  <li><a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/upgrade-best-practices#check_ca_certificates"><span class="text-tiny"><label class="todo-list__label"><input type="checkbox" disabled="disabled"><span class="todo-list__label__description">Check if CA certificates need to be rotated.</span></label></span></a></li>
</ul>
<h6>Downtime</h6>
<ul>
  <li>During the upgrade Anthos drains the nodes, and recreates them</li>
  <li>To calculate a rough estimate for the upgrade time, multiply 15 minutes times the number of nodes in the largest node pool. For example, if you have 10 nodes in the largest pool, the total upgrade time would be about 15 * 10 = 150 minutes or 2.5 hours</li>
  <li>Accelerate an upgrade by setting the value of <a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/user-cluster-configuration-file-latest#nodepool-update-policy-updatestrategy-rolling-update-maxsurge-field"><code>maxSurge</code></a> for individual node pools.</li>
</ul>
<h6>Back up the user cluster</h6>
<ul>
  <li>If the cluster is kubeception</li>
</ul>
<pre><code class="language-plaintext">kubectl --kubeconfig ADMIN_CLUSTER_KUBECONFIG exec -it \
 kube-etcd-0 -c kube-etcd -n USER_CLUSTER_NAME \
 -- /bin/sh</code></pre>
<ul>
  <li>take the snapshot</li>
</ul>
<pre><code class="language-plaintext">ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etcd.local.config/certificates/etcdCA.crt \
  --cert=/etcd.local.config/certificates/etcd.crt \
  --key=/etcd.local.config/certificates/etcd.key \
  snapshot save /tmp/snapshot.db</code></pre>
<ul>
  <li>copy the snapshot into the admin workstation</li>
</ul>
<pre><code class="language-plaintext">kubectl --kubeconfig ADMIN_CLUSTER_KUBECONFIG cp \
  USER_CLUSTER_NAME/kube-etcd-0:/tmp/snapshot.db \
  --container kube-etcd snapshot.db</code></pre>
<ul>
  <li>check if the cluster is controlplane V2</li>
</ul>
<pre><code class="language-plaintext">kubectl get onpremuserclusters --kubeconfig USER_CLUSTER_KUBECONFIG \
  -n kube-system -o jsonpath='{.items[0].spec.enableControlplaneV2}' &amp;&amp; echo</code></pre>
<p>&nbsp;</p>
<ul>
  <li>Get the etcd Pod's name:</li>
</ul>
<pre><code class="language-plaintext">kubectl --kubeconfig USER_CLUSTER_KUBECONFIG get pods \
 -n kube-system -l component=etcd,tier=control-plane -ojsonpath='{$.items[*].metadata.name}{"\n"}'</code></pre>
<p>&nbsp;</p>
<ul>
  <li>Get a shell into the <code>etcd</code> container:</li>
</ul>
<pre><code class="language-plaintext">kubectl --kubeconfig USER_CLUSTER_KUBECONFIG exec -it \
 POD_NAME -c etcd -n kube-system -- /bin/sh
</code></pre>
<ul>
  <li>In your shell, create a backup file named <code>snapshot.db</code>:</li>
</ul>
<pre><code class="language-plaintext">ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /tmp/snapshot.db</code></pre>
<p>&nbsp;</p>
<ul>
  <li>Copy <code>snapshot.db</code> from the <code>etcd</code> container to the workstation home directory:</li>
</ul>
<pre><code class="language-plaintext">kubectl --kubeconfig USER_CLUSTER_KUBECONFIG \
 cp POD_NAME:/tmp/snapshot.db ~/snapshot.db \
 -c etcd -n kube-system</code></pre>
<p>&nbsp;</p>
<ul>
  <li>Or, copy the secrets from inside the etcd pods</li>
</ul>
<pre><code class="language-plaintext">kubectl --kubeconfig ~/kubeconfig  cp etcd-node01:/etc/kubernetes/pki/etcd .  -c etcd -n kube-system</code></pre>
<p>&nbsp;</p>
<ul>
  <li>Copy the secrets from the PKI directory:</li>
</ul>
<pre><code class="language-plaintext">ssh -i NODE_NAME.key ubuntu@NODE_EXTERNAL_IP
sudo chmod -R 0644 /etc/kubernetes/pki/*
sudo chmod 0755 /etc/kubernetes/pki/etcd
exit
scp -ri NODE_NAME.key ubuntu@NODE_EXTERNAL_IP:/etc/kubernetes/pki ~/pki_NODE_NAME</code></pre>
<h6>Backup admin cluster</h6>
<ul>
  <li>Edit the admin cluster configuration file and add the <code>clusterBackup.datastore</code> field, then run <code>gkectl update admin</code>.</li>
</ul>
<pre><code class="language-plaintext">gkectl  backup admin --kubeconfig ADMIN_CLUSTER_KUBECONFIG --config ADMIN_CLUSTER_CONFIG</code></pre>
<h6>Review the use of <code>PodDisruptionBudgets</code></h6>
<pre><code class="language-plaintext">kubectl get pdb -A --kubeconfig KUBECONFIG</code></pre>
<p>&nbsp;</p>
<blockquote>
  <p>Check for PDBs that can't be fulfilled. For example, you might set a minimum availability of 1, when the Deployment only features 1 replica. In this example, the draining operation is disrupted because the PDB can't be satisfied by the resource controller.</p>
</blockquote>
<h6>Review the available IP addresses</h6>
<ul>
  <li>Always have N+1 IP addresses for the cluster, where N is the number of nodes in the user cluster.</li>
  <li>When using static IP addresses, the required IP addresses must be listed in the IP block files.<ul>
      <li>When upgrading non-HA admin clusters, add the additional IP address in the IP block file used by the admin cluster. The path to this file must be specified in the <a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/admin-cluster-configuration-file-latest#network-ipmode.ipblockfilepath-field"><code>network.ipMode.ipBlockFilePath</code></a> field of the admin cluster configuration file.</li>
      <li>When upgrading user clusters, add the additional IP address in the IP block file used by the user cluster. The path to this file must be specified in the <a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/user-cluster-configuration-file-latest#network-ipmode.ipblockfilepath-field"><code>network.ipMode.ipBlockFilePath</code></a> field of the user cluster configuration file.</li>
      <li>If you need to add IP addresses, update the IP block file, then run the <code>gkectl update</code> command.</li>
    </ul>
  </li>
  <li>HA admin clusters don't require N+1 IP addresses for upgrades. The three control-plane nodes in an HA admin cluster are recreated one by one, ensuring no additional IP addresses are needed.</li>
</ul>
<pre><code class="language-plaintext">gkectl check-config --kubeconfig ADMIN_CLUSTER_KUBECONFIG --config USER_CLUSTER_CONFIG</code></pre>
<h6>Check cluster and VmWare utilization</h6>
<ul>
  <li>It's still important to check for free resources in vSphere before you begin an admin cluster upgrade.</li>
  <li>As a general rule, your vSphere cluster must be able to support the following additional resources:<ul>
      <li>+1 VM per admin cluster upgrade</li>
      <li>+1 VM per node pool per user cluster upgrade</li>
      <li>For example, assume that a user cluster has 3 node pools where each node pool has nodes using 8 vCPUs and 32GB or more of RAM. Because the upgrade happens in parallel for the 3 node pools by default, the upgrade procedure consumes the following additional resources for the 3 additional surge nodes:<ul>
          <li>24 vCPUs</li>
          <li>96GB of RAM</li>
          <li>VM disk space + 96GB of vSwap</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<h6>Check the cluster health and configuration</h6>
<ul>
  <li>The <code>gkectl diagnose</code> command: <code>gkectl diagnose</code> ensures all clusters are healthy. The command runs advanced checks, such as to identify nodes that aren't configured properly, or that have Pods that are in a stuck state. If the <code>gkectl diagnose</code> command shows a <code>Cluster unhealthy</code> warning, fix the issues before you attempt an upgrade.</li>
  <li>Run the <code>gkectl upgrade cluster</code> command with the <code>--dry-run</code> flag. The <code>--dry-run</code> flag runs <a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/upgrading#run-preflight-checks">preflight checks</a> but doesn't start the upgrade process.</li>
  <li>The <a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/pre-upgrade-tool">pre-upgrade tool</a>: in addition to checking the cluster health and configuration, the pre-upgrade tool checks for potential known issues that could happen during a cluster upgrade.</li>
</ul>
<h6>Check user and admin cluster versions</h6>
<ul>
  <li>Check the version of the user clusters:</li>
</ul>
<pre><code class="language-plaintext">gkectl list clusters --kubeconfig ADMIN_CLUSTER_KUBECONFIG</code></pre>
<p>&nbsp;</p>
<ul>
  <li>check the version of the admin clusters</li>
</ul>
<pre><code class="language-plaintext">gkectl list admin --kubeconfig ADMIN_CLUSTER_KUBECONFIG</code></pre>
<p>&nbsp;</p>
<ul>
  <li>check the version of the user cluster nodes</li>
</ul>
<pre><code class="language-plaintext">kubectl get nodes --kubeconfig USER_CLUSTER_KUBECONFIG</code></pre>
<h6>Check if CA certificates need to be rotated</h6>
<ul>
  <li>During an upgrade, leaf certificates are rotated, but CA certificates aren't. You must manually rotate your CA certificates at least once every five years.</li>
</ul>
<h3>Run the pre-upgrade tool</h3>
<ul>
  <li>Upgrade the admin workstation</li>
  <li>Run <code>gkectl prepare</code> to import OS images to vSphere if you have not done so:</li>
</ul>
<pre><code class="language-plaintext">gkectl prepare \
    --bundle-path /var/lib/gke/bundles/gke-onprem-vsphere-TARGET_VERSION.tgz \
    --kubeconfig ADMIN_CLUSTER_KUBECONFIG
    --advanced-cluster=false</code></pre>
<p>&nbsp;</p>
<ul>
  <li>In the following bash script, set values for these placeholders:<ul>
      <li><code>ADMIN_CLUSTER_KUBECONFIG</code>: The path to the admin cluster kubeconfig.</li>
      <li><code>REGISTRY_ADDRESS</code>: If the admin cluster uses a private registry, this is the private registry address that you specified in the previous step. If you aren't using a private registry, specify the public registry: <code>gcr.io/gke-on-prem-release</code></li>
    </ul>
  </li>
</ul>
<pre><code class="language-plaintext">#!/bin/bash
UPGRADE_TARGET_VERSION=${1}
CLUSTER_NAME=${2}
ADMIN_KUBECONFIG=ADMIN_CLUSTER_KUBECONFIG
REGISTRY_ADDRESS=REGISTRY_ADDRESS
pre_upgrade_namespace=kube-system
if [[ -z "$CLUSTER_NAME" ]]
then
  echo "Running the pre-ugprade tool before admin cluster upgrade"
else
  echo "Running the pre-ugprade tool before user cluster upgrade"
  pre_upgrade_namespace=$CLUSTER_NAME-gke-onprem-mgmt
fi
kubectl apply --kubeconfig ${ADMIN_KUBECONFIG} -f - &lt;&lt;EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: pre-upgrade-job
  namespace: $pre_upgrade_namespace
EOF
kubectl apply --kubeconfig ${ADMIN_KUBECONFIG} -f - &lt;&lt;EOF
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  creationTimestamp: null
  name: pre-upgrade-job-rolebinding-in-$pre_upgrade_namespace
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: onprem-user-cluster-controller-role
subjects:
  - kind: ServiceAccount
    name: pre-upgrade-job
    namespace: $pre_upgrade_namespace
EOF
kubectl apply --kubeconfig ${ADMIN_KUBECONFIG} -f - &lt;&lt;EOF
apiVersion: batch/v1
kind: Job
metadata:
  name: pre-upgrade-$(date +%Y-%m%d-%H%M%S)
  namespace: $pre_upgrade_namespace
  labels:
    onprem.cluster.gke.io/job-usage: preflight
spec:
  ttlSecondsAfterFinished: 2592000
  backoffLimit: 2
  template:
    metadata:
      labels:
        onprem.cluster.gke.io/pod-usage: preflight
    spec:
      containers:
      - name: preflight
        image: $REGISTRY_ADDRESS/preflight@sha256:9704315c6637750a014d0079ca04a8f97d0ca3735e175020377107c3181f6234
        imagePullPolicy: Always
        command:
        - /preflight
        - --upgrade-target-version
        - "$UPGRADE_TARGET_VERSION"
        - --cluster-name
        - "$CLUSTER_NAME"
        - --scenario
        - pre-upgrade
      restartPolicy: Never
      serviceAccountName: pre-upgrade-job
      imagePullSecrets:
      - name: private-registry-creds
EOF</code></pre>
<p>&nbsp;</p>
<pre><code class="language-plaintext">chmod +x pre-upgrade.sh</code></pre>
<p>&nbsp;</p>
<ul>
  <li>for admin cluster</li>
</ul>
<pre><code class="language-plaintext">./pre-upgrade.sh TARGET_VERSION</code></pre>
<p>&nbsp;</p>
<ul>
  <li>for user clusters</li>
</ul>
<pre><code class="language-plaintext">./pre-upgrade.sh TARGET_VERSION USER_CLUSTER_NAME</code></pre>
<p>&nbsp;</p>
<ul>
  <li>get the logs of the job</li>
</ul>
<pre><code class="language-plaintext">kubectl logs -n JOB_NAMESPACE jobs/JOB_NAME \
    --kubeconfig ADMIN_CLUSTER_KUBECONFIG</code></pre>
<h6>advanced clusters</h6>
<p>* &nbsp;When you upgrade your clusters from version 1.32 to 1.33, they are automatically converted to advanced clusters</p>
<pre><code class="language-plaintext">Set enableAdvancedCluster in your admin cluster configuration file to false.</code></pre>
<p>&nbsp;</p>
<blockquote>
  <p>When you upgrade an admin cluster, Google Distributed Cloud deploys a <a href="https://kind.sigs.k8s.io/">Kubernetes in Docker</a> (kind) cluster to temporarily host the Kubernetes controllers needed to upgrade the admin cluster. This transient cluster is called a <i>bootstrap cluster</i>. Server-side preflight checks are run on the bootstrap cluster when you upgrade an admin cluster.</p>
</blockquote>
<h3>Cluster upgrade</h3>
<ul>
  <li>Remove any non-system NetworkPolicy</li>
</ul>
<pre><code class="language-plaintext">kubectl --kubeconfig USER_CLUSTER_KUBECONFIG get networkpolicy -A -o wide | grep -v kube-system</code></pre>
<p>&nbsp;</p>
<ul>
  <li>backup all the non-system NetworkPolicies</li>
</ul>
<pre><code class="language-plaintext">kubectl --kubeconfig USER_CLUSTER_KUBECONFIG get networkpolicy NETWORK_POLICY_NAME -n NETWORK_POLICY_NAMESPACE -o yaml &gt; NETWORK_POLICY_NAME.yaml</code></pre>
<p>&nbsp;</p>
<ul>
  <li>delete the non-system NetworkPolicies</li>
</ul>
<pre><code class="language-plaintext">kubectl --kubeconfig USER_CLUSTER_KUBECONFIG delete networkpolicy NETWORK_POLICY_NAME -n NETWORK_POLICY_NAMESPACE</code></pre>
<p>&nbsp;</p>
<ul>
  <li>after the upgrade restore the networkpolicies</li>
</ul>
<h6>Check available versions for cluster upgrades</h6>
<pre><code class="language-plaintext">gkectl version --kubeconfig ADMIN_CLUSTER_KUBECONFIG</code></pre>
<h2>Upgrade your admin workstation</h2>
<ul>
  <li>get the INFO_FILE from the admin workstation</li>
  <li>ensure that the admin-ws-cofig.yaml exist in the jumb server</li>
  <li>list the available versions of gkeadm</li>
</ul>
<blockquote>
  <p>When upgrading to 1.28 and higher, 100 GB is required, and the cluster upgrade fails if the admin workstation doesn't have sufficient disk space.</p>
</blockquote>
<pre><code class="language-plaintext">gcloud storage ls gs://gke-on-prem-release/gkeadm/v1.31.12-gke.600/linux/</code></pre>
<p>&nbsp;</p>
<ul>
  <li>Upgrade your admin workstation</li>
</ul>
<pre><code class="language-plaintext">gkeadm upgrade admin-workstation --config AW_CONFIG_FILE \
    --info-file INFO_FILE</code></pre>
<h2>Upgrade the admin cluster</h2>
<ul>
  <li>The admin cluster version, including the patch version, must be greater than or equal to the user cluster version.</li>
  <li>From the admin workstation run:</li>
</ul>
<pre><code class="language-plaintext">gkectl prepare \
    --bundle-path /var/lib/gke/bundles/gke-onprem-vsphere-TARGET_VERSION.tgz \
    --kubeconfig ADMIN_CLUSTER_KUBECONFIG</code></pre>
<p>&nbsp;</p>
<ul>
  <li>On your admin workstation, start an asynchronous upgrade:</li>
</ul>
<pre><code class="language-plaintext">gkectl upgrade admin \
    --kubeconfig ADMIN_CLUSTER_KUBECONFIG \
    --config ADMIN_CLUSTER_CONFIG_FILE \
    --async || optinal</code></pre>
<p>&nbsp;</p>
<ul>
  <li>To see the status of the upgrade:</li>
</ul>
<pre><code class="language-plaintext">gkectl list admin --kubeconfig ADMIN_CLUSTER_KUBECONFIG</code></pre>
<p>&nbsp;</p>
<ul>
  <li>To get more details about the upgrade progress and cluster events:</li>
</ul>
<pre><code class="language-plaintext">gkectl describe admin --kubeconfig ADMIN_CLUSTER_KUBECONFIG</code></pre>
<p>&nbsp;</p>
<blockquote>
  <p>If you are upgrading to version 1.14.0 or higher, a new kubeconfig file is generated for the admin cluster that overwrites any existing file. To view cluster details in the file, run the following command:</p>
  <pre><code class="language-plaintext">  kubectl config view --kubeconfig ADMIN_CLUSTER_KUBECONFIG</code></pre>
  <p>&nbsp;</p>
</blockquote>
<h2>Upgrade a user cluster</h2>
<ul>
  <li>In the user cluster configuration file, set <a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/user-cluster-configuration-file-latest#gkeonpremversion-field"><code>gkeOnPremVersion</code></a> to the target version of your upgrade.</li>
</ul>
<blockquote>
  <p>If your cluster has a Windows node pool, run <code>gkectl prepare windows</code>, and update the <code>osImage</code> field for the node pool.&nbsp;</p>
</blockquote>
<pre><code class="language-plaintext">gkectl upgrade cluster \
    --kubeconfig ADMIN_CLUSTER_KUBECONFIG \
    --config USER_CLUSTER_CONFIG \
    --dry-run</code></pre>
<p>&nbsp;</p>
<pre><code class="language-plaintext">gkectl upgrade cluster \
  --kubeconfig ADMIN_CLUSTER_KUBECONFIG \
  --config USER_CLUSTER_CONFIG</code></pre>
<p>&nbsp;</p>
<pre><code class="language-plaintext">kubectl config view --kubeconfig USER_CLUSTER_KUBECONFIG</code></pre>
<p>&nbsp;</p>
<ul>
  <li>If a user cluster upgrade is interrupted, you can resume the user cluster upgrade by running the same upgrade command with the <code>--skip-validation-all</code> flag:</li>
</ul>
<pre><code class="language-plaintext">gkectl upgrade cluster \
    --kubeconfig ADMIN_CLUSTER_KUBECONFIG \
    --config USER_CLUSTER_CONFIG \
    --skip-validation-all</code></pre>
<h2>Resuming an admin cluster upgrade</h2>
<ul>
  <li>If <code>gkectl</code> exits unexpectedly during an admin cluster upgrade, the kind cluster is not cleaned up. Before you rerun the upgrade command to resume the upgrade, delete the kind cluster:</li>
</ul>
<pre><code class="language-plaintext">docker stop gkectl-control-plane &amp;&amp; docker rm gkectl-control-plane</code></pre>
<h2>Upgrade node pools</h2>
<ul>
  <li>Define the source version and the target version in the following placeholder variables. All versions must be the full version number in the form <code>x.y.z-gke.N</code> such as <code>1.16.11-gke.25</code>.</li>
  <li>import the corresponding OS images to vSphere:</li>
</ul>
<pre><code class="language-plaintext">gkectl prepare \
  --bundle-path /var/lib/gke/bundles/gke-onprem-vsphere-TARGET_VERSION.tgz \
  --kubeconfig ADMIN_CLUSTER_KUBECONFIG</code></pre>
<p>&nbsp;</p>
<ul>
  <li>Make the following changes in the user cluster configuration file:</li>
  <li>Set the <code>gkeOnPremVersion</code> field to the target version, <code>TARGET_VERSION</code>.<ul>
      <li>For each node pool that you want to upgrade, set the <code>nodePools.nodePool[i].gkeOnPremVersion</code> field to the empty string.<ul>
          <li>In version 1.28 and later, you can accelerate the node pool upgrade by setting <code>nodePools.nodePool[i].updateStrategy.rollingUpdate.maxSurge</code> <a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/user-cluster-configuration-file-latest#nodepool-updatestrategy-rollingupdate-maxsurge-field">field</a> to an integer value greater than 1. When you upgrade nodes with <code>maxSurge</code>, multiple nodes upgrade in the same time that it takes to upgrade a single node.</li>
        </ul>
      </li>
      <li>For each node pool that you <strong>don't</strong> want to upgrade, set <code>nodePools.nodePool[i].gkeOnPremVersion</code> to the source version, <code>SOURCE_VERSION</code>.</li>
    </ul>
  </li>
</ul>
<pre><code class="language-plaintext">gkeOnPremVersion: TARGET_VERSION
...
nodePools:
- name: pool-1
  gkeOnPremVersion: ""
  ...
- name: pool-2
  gkeOnPremVersion: SOURCE_VERSION
  ...</code></pre>
<p>&nbsp;</p>
<ul>
  <li>Upgrade the control plane and selected node pools:</li>
</ul>
<pre><code class="language-plaintext">gkectl upgrade cluster \
  --kubeconfig ADMIN_CLUSTER_KUBECONFIG \
  --config USER_CLUSTER_CONFIG_FILE</code></pre>
<p>&nbsp;</p>
<p>&nbsp;</p>
