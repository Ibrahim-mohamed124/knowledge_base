<!--
title: Anthos Recommended Features Migration
description: 
published: true
date: 2026-02-16T13:03:51.496Z
tags: 
editor: ckeditor
dateCreated: 2026-02-15T19:53:45.620Z
-->

<h1>Admin Cluster</h1>
<h2>Key rotation</h2>
<ul>
  <li>To migrate from Non-HA to HA, first rotate the encryption key if always-on secret encryption is enabled</li>
  <li>To rotate an existing encryption key for a cluster, increment the <code>keyVersion</code> in the corresponding <a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/admin-cluster-configuration-file-latest#secretsencryption-generatedkey-version-field">admin cluster configuration file</a> or <a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/user-cluster-configuration-file-latest#secretsencryption-generatedkey-version-field">user cluster configuration file</a>, and run the appropriate <code>gkectl update</code> command.</li>
  <li>This creates a new key matching the new version number, re-encrypts each secret, and securely erases the old one. All subsequent new secrets are encrypted using the new encryption key.</li>
</ul>
<pre><code class="language-plaintext">secretsEncryption:
  mode: GeneratedKey
  generatedKey:
    keyVersion: New Version
    disabled: true</code></pre>
<p>&nbsp;</p>
<ul>
  <li>update the cluster</li>
</ul>
<pre><code class="language-plaintext">gkectl update --config cluster-config.yaml --kubeconfig kubeconfig_admin_cluster</code></pre>
<p>&nbsp;</p>
<blockquote>
  <p>The Kubernetes API access is unavailable for non-HA admin clusters for about 20 minutes</p>
</blockquote>
<h2>HA&nbsp;</h2>
<h3>Allocate additional IP addresses</h3>
<ul>
  <li>When migrating the admin cluster from non-HA to HA, allocate four additional IP addresses. Ensure these IP addresses are in the same VLAN as the existing admin cluster nodes and aren't already used by any existing nodes:<ul>
      <li>Allocate one IP address for the new control plane VIP, for the <a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/admin-cluster-configuration-file-latest#loadbalancer-vips-controlplanevip-field"><code>loadBalancer.vips.controlPlaneVIP</code></a> field in the admin cluster configuration file.</li>
      <li>Allocate a new IP addresses for each of the three control-plane nodes, for the <a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/admin-cluster-configuration-file-latest#loadbalancer-vips-controlplanevip-field"><code>network.controlPlaneIPBlock</code></a> section in the admin cluster configuration file.</li>
    </ul>
  </li>
</ul>
<h3>Rotate the Encryption key</h3>
<ul>
  <li>check if the encryption key is not rotated</li>
</ul>
<pre><code class="language-plaintext">kubectl --kubeconfig ADMIN_CLUSTER_KUBECONFIG get secret -n kube-system admin-master-component-options -o jsonpath='{.data.data}' | base64 -d | grep -oP '"GeneratedKeys":\[.*?\]'</code></pre>
<p>&nbsp;</p>
<ul>
  <li>If the output shows an empty key, such as in the following example, you must rotate the encryption key</li>
</ul>
<pre><code class="language-plaintext">"GeneratedKeys":[{"KeyVersion":"1","Key":""}]</code></pre>
<h3>Update the admin cluster configuration file</h3>
<pre><code class="language-plaintext">vCenter:
  address: "my-vcenter-server.my-domain.example"
  datacenter: "my-data-center"
  dataDisk: "xxxx.vmdk"   &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; remove this
...
network:
  hostConfig:
    dnsServers:
    - 203.0.113.1
    - 203.0.113.2
    ntpServers:
    - 203.0.113.3
  ...
  controlPlaneIPBlock:
    netmask: "255.255.255.0"
    gateway: "198.51.100.1"
    ips:
    - ip: "192.0.2.1"
      hostname: "admin-cp-hostname-1"
    - ip: "192.0.2.2"
      hostname: "admin-cp-hostname-2"
    - ip: "192.0.2.3"
      hostname: "admin-cp-hostname-3"
...

...
loadBalancer:
  vips:
    controlPlaneVIP:  192.0.2.50


...
adminMaster:
  replicas: 3
  cpus: 4
  memoryMB: 8192
...</code></pre>
<h3>Migrate the admin cluster</h3>
<pre><code class="language-plaintext">gkectl update admin --kubeconfig ADMIN_CLUSTER_KUBECONFIG \
    --config ADMIN_CLUSTER_CONFIG</code></pre>
<p>&nbsp;</p>
<blockquote>
  <p>During the migration from non-HA to HA, the older control-plane VIP continues to function and can be used to access the new HA admin cluster. When the migration completes, the admin cluster kubeconfig file is automatically updated to use the new control-plane VIP.</p>
</blockquote>
<h1>User Cluster</h1>
<blockquote>
  <p>For any other update in the migration process, multiply 15 minutes by the number of nodes in the node pool.</p>
</blockquote>
<ul>
  <li>The following high-level steps show when you need to run <code>gkectl update cluster</code> to update the cluster:</li>
</ul>
<ol>
  <li>If the user cluster uses <a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/always-on-secrets-encryption#enable_always-on_secrets_encryption">always-on secret encryption</a>, disable the feature and run <code>gkectl update cluster</code>.</li>
</ol>
<ul>
  <li>Prepare for load balancer and control plane migration:</li>
</ul>
<ol>
  <li>If the admin cluster has <a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/node-auto-repair#enabling_node_repair_and_health_checking_for_an_existing_admin_cluster">auto-repair enabled</a>, disable it. Then run <code>gkectl update admin</code>. This update finishes quickly because it does not recreate the admin cluster nodes.</li>
</ol>
<ul>
  <li>Make all the needed configuration changes to update your load balancer and to migrate to Controlplane V2. Then run <code>gkectl update cluster</code>.</li>
  <li>After the migration, if you disabled always-on secret encryption, re-enable the feature and run <code>gkectl update cluster</code>.</li>
</ul>
<blockquote>
  <p>150 minutes for the Controlplane V2 and MetalLB update because 15 minutes * 10 nodes in the biggest pool = 150 minutes.</p>
</blockquote>
<h3>Plan for downtime during migration</h3>
<ul>
  <li>When planning your migration, plan for these types of downtime:<ul>
      <li><strong>Control-plane downtime</strong>: Access to the Kubernetes API server is affected during the migration. If you are migrating to Controlplane V2, there is control-plane downtime for kubeception user clusters as the <a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/user-cluster-configuration-file-latest#loadbalancer-vips-controlplanevip-field"><code>loadBalancer.vips.controlPlaneVIP</code></a> is migrated. The downtime is typically be less than 10 minutes, but the length of the downtime depends on your infrastructure.</li>
      <li><strong>Workload downtime</strong>: The virtual IPs (VIPs) used by <a href="https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer">Services of type: LoadBalancer</a> are unavailable. This only occurs during a migration from Seesaw to MetalLB. The MetalLB migration process will stop network connections to all VIPs in the user cluster for Kubernetes Services of type LoadBalancer for about <mark class="marker-yellow">two to ten minutes.</mark> After the migration is complete, the connections work again.</li>
    </ul>
  </li>
</ul>
<h3>Prepare for the migration</h3>
<ul>
  <li>If you are migrating to Controlplane V2, allocate new static IP addresses in the same VLAN as the worker nodes (the nodes in node pools).</li>
  <li>You need one IP address for the <a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/user-cluster-configuration-file-latest#loadbalancer-vips-controlplanevip-field"><code>loadBalancer.vips.controlPlaneVIP</code></a>.</li>
  <li>Allocate one IP address for each control-plane node. The number of IP addresses that you need to allocate depends on whether the user cluster will be highly available (HA) or non-HA.<ul>
      <li>Non-HA: one IP address</li>
      <li>HA: three IP addresses</li>
    </ul>
  </li>
</ul>
<h6>Check the cluster and node pool versions</h6>
<pre><code class="language-plaintext">gkectl version --kubeconfig ADMIN_CLUSTER_KUBECONFIG --details</code></pre>
<h3>Check the cluster health</h3>
<pre><code class="language-plaintext">gkectl diagnose cluster --kubeconfig ADMIN_CLUSTER_KUBECONFIG \
    --cluster-name USER_CLUSTER_NAME</code></pre>
<h6>Disable auto-repair in the admin cluster</h6>
<pre><code class="language-plaintext">autoRepair:
  enabled: false
</code></pre>
<pre><code class="language-plaintext">gkectl update admin --kubeconfig ADMIN_CLUSTER_KUBECONFIG \
    --config ADMIN_CLUSTER_CONFIG</code></pre>
<h6>Check for an issue with always-on secret encryption</h6>
<pre><code class="language-plaintext">kubectl --kubeconfig ADMIN_CLUSTER_KUBECONFIG \
  get onpremusercluster USER_CLUSTER_NAME \
  -n USER_CLUSTER_NAME-gke-onprem-mgmt \
  -o jsonpath={.spec.secretsEncryption}</code></pre>
<ul>
  <li>you must do the steps in the next section to ensure that the new Controlplane V2 cluster can decrypt secrets.</li>
</ul>
<h4>Disable always-on secrets encryption and decrypt secrets if needed</h4>
<ul>
  <li>In the user cluster configuration file, to disable always-on secrets encryption add a <code>disabled: true</code> field to the <code>secretsEncryption</code> section:</li>
</ul>
<pre><code class="language-plaintext">secretsEncryption:
    mode: GeneratedKey
    generatedKey:
        keyVersion: KEY_VERSION
        disabled: true</code></pre>
<ul>
  <li>Update the cluster:</li>
</ul>
<pre><code class="language-plaintext">gkectl update cluster --kubeconfig ADMIN_CLUSTER_KUBECONFIG \
    --config USER_CLUSTER_CONFIG</code></pre>
<ul>
  <li>Do a rolling update on a specific DaemonSet, as follows:</li>
</ul>
<pre><code class="language-plaintext">kubectl --kubeconfig ADMIN_CLUSTER_KUBECONFIG \
  rollout restart statefulsets kube-apiserver \
  -n USER_CLUSTER_NAME</code></pre>
<ul>
  <li>Get the manifests of all the secrets in the user cluster, in YAML format:</li>
</ul>
<pre><code class="language-plaintext">kubectl --kubeconfig USER_CLUSTER_KUBECONFIG \
  get secrets -A -o yaml &gt; SECRETS_MANIFEST.yaml</code></pre>
<ul>
  <li>So that all secrets are stored in etcd as plaintext, reapply all the secrets in the user cluster:</li>
</ul>
<pre><code class="language-plaintext">kubectl --kubeconfig USER_CLUSTER_KUBECONFIG \
  apply -f SECRETS_MANIFEST.yaml</code></pre>
<h3>Prepare for the migration to Controlplane V2</h3>
<h6>Update the user cluster configuration file</h6>
<ol>
  <li>Set <a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/user-cluster-configuration-file-latest#enablecontrolplanev2-field"><code>enableControlplaneV2</code></a> to true.</li>
  <li>Optionally, make the control plane for the Controlplane V2 user cluster highly available (HA). To change from a non-HA to an HA cluster, change <a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/user-cluster-configuration-file-latest#masternode-replicas-field"><code>masterNode.replicas</code></a> from 1 to 3.</li>
  <li>Add the static IP address (or addresses) for the user cluster control-plane node(s) to the <a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/user-cluster-configuration-file-latest#network-controlplaneipblock-ips-section"><code>network.controlPlaneIPBlock.ips</code></a> section. The IP address (or addresses) for the control-plane nodes must be in the same VLAN as the worker nodes. The hostnames are required.</li>
  <li>Fill in the <code>netmask</code> and <code>gateway</code> in the <a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/user-cluster-configuration-file-latest#network-controlplaneipblock-section"><code>network.controlPlaneIPBlock</code></a> section.</li>
  <li>If the <a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/user-cluster-configuration-file-latest#network-hostconfig-section"><code>network.hostConfig</code></a> section is empty, fill it in.</li>
  <li>Make sure that the <a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/user-cluster-configuration-file-latest#loadbalancer-vips-controlplanevip-field"><code>loadBalancer.vips.controlPlaneVIP</code></a> field has the new IP address for the control plane VIP. The IP address has to be in the same VLAN as the control plane node IPs.</li>
  <li>If the user cluster uses manual load balancing, set <a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/user-cluster-configuration-file-latest#loadbalancer-manuallb.controlplanenodeport-field"><code>loadBalancer.manualLB.controlPlaneNodePort</code></a> and <a href="https://docs.cloud.google.com/kubernetes-engine/distributed-cloud/vmware/docs/how-to/user-cluster-configuration-file-latest#loadbalancer-manuallb.konnectivityservernodeport-field"><code>loadBalancer.manualLB.konnectivityServerNodePort</code></a> to 0. They are not required when Controlplane V2 is enabled but they must have 0 as a value.</li>
</ol>
<pre><code class="language-plaintext">gkectl update cluster --kubeconfig ADMIN_CLUSTER_KUBECONFIG \
    --config USER_CLUSTER_CONFIG</code></pre>
<h3>Controlplane V2 migration</h3>
<ul>
  <li>During the Controlplane V2 migration, the update performs the following actions:</li>
</ul>
<ol>
  <li>Creates the control plane of a new cluster with ControlPlane V2 enabled.</li>
  <li>Stops the kubeception cluster's Kubernetes control plane.</li>
  <li>Takes an etcd snapshot of the kubeception cluster.</li>
  <li>Powers off the kubeception cluster's user cluster control plane nodes. Until the migration completes, the nodes are not deleted because that permits failure recovery by falling back to that kubeception cluster.</li>
  <li>Restores the cluster data in the new control plane, using the etcd snapshot created in an earlier step.</li>
  <li>Connects the nodepool nodes of the kubeception cluster to the new control-plane, which is accessible with the new <code>controlPlaneVIP</code>.</li>
  <li>Reconciles the restored user cluster to meet the end state of the cluster with ControlPlane V2 enabled.</li>
</ol>
